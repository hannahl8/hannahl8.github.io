{
  "hash": "1bdc49c01be0de02d18516bc9da13e3d",
  "result": {
    "markdown": "---\ntitle: 'Classification'\ndate: '2023-10-30'\ncategories: ['code']\ndescription: 'ROC, PR, Confusion Matrix'\nexecute: \n  message: false\n  warning: false\neditor_options: \n  chunk_output_type: console\n---\n\n## Introduction\n\nClassification, a cornerstone of machine learning, empowers systems to make informed decisions based on input features. From determining whether an email is spam to diagnosing diseases, classification algorithms play a pivotal role in automating decision-making processes.\n\n### Types of Classification Algorithms\n\nThere are several classification algorithms, each suited to different types of problems:\n\n-   **Logistic Regression:** Ideal for binary classification tasks.\n\n-   **Decision Trees:** Effective for both binary and multiclass classification.\n\n-   **Support Vector Machines (SVM):** Robust for linear and nonlinear classification.\n\nLet's implement a simple classification model using logistic regression in Python:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n\n# Generate synthetic classification data\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = (X > 0.5).astype(int).flatten()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_proba = logreg.predict_proba(X_test)[:, 1]\n```\n:::\n\n\n## Data Visualization\n\nI use the following colors in all of my blogs data visualizations\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n```\n:::\n\n\n### **Receiver Operating Characteristic (ROC) Curve**\n\nROC curves visualize the trade-off between true positive rate (sensitivity) and false positive rate. The area under the ROC curve (AUC-ROC) is a valuable metric for model performance.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Visualize the ROC curve\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nfig, ax = plt.subplots()\nax.plot(fpr, tpr, color=blue, lw=2, label='ROC curve')\nax.plot([0, 1], [0, 1], color=purple, lw=2, linestyle='--', label='Random Guess')\nax.set_xlabel('False Positive Rate', color=blue)\nax.set_ylabel('True Positive Rate', color=blue)\nax.set_title('Receiver Operating Characteristic (ROC) Curve', color=purple)\nax.legend(loc='lower right')\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=589 height=449}\n:::\n:::\n\n\n### **Precision-Recall (PR) Curve**\n\nPR curves focus on the trade-off between precision and recall, particularly valuable in imbalanced datasets.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Visualize the Precision-Recall curve\nprecision, recall, _ = precision_recall_curve(y_test, y_proba)\nfig, ax = plt.subplots()\nax.plot(recall, precision, color=blue, lw=2, label='Precision-Recall curve')\nax.set_xlabel('Recall (Sensitivity)', color=blue)\nax.set_ylabel('Precision', color=blue)\nax.set_title('Precision-Recall Curve', color=purple)\nax.legend(loc='lower left')\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=589 height=449}\n:::\n:::\n\n\n### **Confusion Matrix**\n\nThe confusion matrix provides a detailed understanding of a classification model's performance, breaking down predictions into true positives, true negatives, false positives, and false negatives.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\n\n# Generate predictions\ny_pred = logreg.predict(X_test)\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualize the confusion matrix\nfig, ax = plt.subplots()\ncax = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.cool)\nax.set_title('Confusion Matrix', color=purple)\nplt.colorbar(cax)\nclasses = ['Class 0', 'Class 1']\ntick_marks = np.arange(len(classes))\nax.set_xticks(tick_marks)\nax.set_yticks(tick_marks)\nax.set_xticklabels(classes, rotation=45, color=blue)\nax.set_yticklabels(classes, color=blue)\nax.set_xlabel('Predicted label', color=blue)\nax.set_ylabel('True label', color=blue)\nfig.set_facecolor(background)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=535 height=479}\n:::\n:::\n\n\nIn conclusion, classification in machine learning is a powerful tool for automating decision-making processes. By implementing and evaluating classification models, we gain insights into their performance through metrics like ROC curves, PR curves, and confusion matrices. These visualizations provide a nuanced understanding of a model's strengths and weaknesses, facilitating informed decision-making in real-world applications.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}