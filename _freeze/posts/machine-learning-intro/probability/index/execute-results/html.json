{
  "hash": "4a0c50b38263f8ad575b9ee2a53388bf",
  "result": {
    "markdown": "---\ntitle: 'Probability Theory and Random Variables'\ndate: '2023-09-20'\ncategories: ['code']\ndescription: 'Understanding Probability Theory and Random Variables in the Context of Machine Learning'\nexecute: \n  message: false\n  warning: false\neditor_options: \n  chunk_output_type: console\n---\n\n## Introduction\n\nIn the realm of machine learning, uncertainty is inherent. Models make predictions based on patterns learned from data, and probability theory provides a formal framework for dealing with uncertainty. Random variables are employed to represent uncertain quantities, making probability theory a cornerstone for building and interpreting machine learning models.\n\nMachine learning algorithms often assume specific probability distributions. For instance:\n\n-   The normal distribution is frequently assumed in linear regression and Gaussian processes.\n\n-   The binomial distribution is relevant for problems involving binary outcomes, such as classification tasks.\n\nUnderstanding the characteristics of these distributions is vital when selecting appropriate models and interpreting results.\n\nRandom variables represent various aspects of data. Features can be modeled as random variables, and understanding their distribution is crucial for feature engineering. Additionally, the uncertainty associated with predictions is often expressed through probability distributions, making random variables central to the prediction process.\n\n## Data Visualization\n\nI use the following colors in all of my blogs data visualizations\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n```\n:::\n\n\nI will use the following **`plot_histograms`** function to create the data visualizations on this page\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Function to plot histograms\ndef plot_histograms(data, title):\n    # Create a histogram\n    fig, ax = plt.subplots()\n    hist = ax.hist(data, bins=30, edgecolor='black', alpha=0.7, color=purple, ec=blue)\n    ax.set_title(title, color=purple)\n    ax.set_xlabel('Values', color=blue)\n    ax.set_ylabel('Frequency', color=blue)\n    ax.grid(True, linestyle='--', alpha=0.5)\n    ax.tick_params(axis='x', colors=blue)\n    ax.tick_params(axis='y', colors=blue)\n    \n    ax.set_facecolor(pink)\n    fig.set_facecolor(background)\n    \n    # Show the plot\n    plt.show()\n```\n:::\n\n\nLet's use Python to create a histogram of a simulated random variable:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\n\n# Sets the seed for the random number generator to 42\nnp.random.seed(42)\n# Generates an array of 1000 random numbers drawn from a normal distribution\n# using the random number generated defined above\ndata = np.random.normal(size=1000)\nplot_histograms(data, 'Histogram of Random Variable')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=593 height=449}\n:::\n:::\n\n\nProbability theory and random variables are not just theoretical concepts; they are integral to the practical application of machine learning. Models are built upon the assumptions of probability distributions, and the visualization of data through histograms aids in model interpretation and validation. As you dive deeper into machine learning, a solid understanding of these foundational concepts will empower you to make more informed decisions in model selection, training, and evaluation.\n\nThe following code generates a dataset with two classes, trains a Random Forest Classifier on the data, and then generates predictions on a test set. The histograms before and after applying the machine learning model are plotted on top of one another for comparison.\n\nThe **`generate_dataset`** function creates a synthetic dataset with two classes, and the **`plot_histograms`** function is used to visualize the distribution before and after applying the machine learning model. The accuracy of the model is also printed to give an idea of its performance.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Function to generate a random dataset with two classes\ndef generate_dataset(seed=42):\n    np.random.seed(seed)\n    class1 = np.random.normal(loc=0, scale=1, size=500)\n    class2 = np.random.normal(loc=3, scale=1, size=500)\n    labels = [0] * 500 + [1] * 500\n    data = np.concatenate([class1, class2])\n    return data, labels\n\n# Step 1: Generate a random dataset\ndata_before, labels = generate_dataset()\n\n# Step 2: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data_before.reshape(-1, 1), labels, test_size=0.2, random_state=42)\n\n# Step 3: Train a Random Forest Classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Step 4: Make predictions on the test set\npredictions = clf.predict(X_test)\n\n# Step 5: Assess accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy of the model: {accuracy:.2%}\")\n\n# Step 6: Generate a dataset after applying the machine learning model\ndata_after = np.concatenate([clf.predict_proba(X_test)[:, 0], clf.predict_proba(X_test)[:, 1]])\n\n# Step 7: Plot histograms to compare before and after\nplot_histograms(data_before, 'Original Dataset')\nplot_histograms(data_after, 'Dataset after Machine Learning')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of the model: 89.00%\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=585 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-3.png){width=593 height=449}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}