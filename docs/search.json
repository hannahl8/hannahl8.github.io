[
  {
    "objectID": "posts/projects/vt-research-connect/index.html",
    "href": "posts/projects/vt-research-connect/index.html",
    "title": "VT Research Connect",
    "section": "",
    "text": "Overview\nThis was my groups capstone project completed for my Master’s program. Please see VTechWorks for more information on the project and a video presentation.\n\n\nLinks to Repositories\nback-end\nfront-end\n\n\nTech Stack\nUI Design: Figma\nBackend: Java (Spring Boot)\nDatabase: PostgreSQL\nFrontend: React, TypeScript, Vite\n\n\nMy Contributions\nI was the front-end lead for this project. I suggested we used React/TypeScript/Vite for the front end code, which integrated well with our Java (Spring Boot) back-end. I completed all of the front-end code, including services, components, contexts, and pages. I also created all of the original UI design using Figma for each of the pages.\nFor a fun little short side piece of the project, I also wrote python code to generate a neat geometric shape for the logo.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntheta = np.linspace(0, 2. * np.pi, 1000)\ncolors = ['white', '#861F41', '#E5751F', '#AB637A']\nfig = plt.figure(figsize=(6, 6), dpi=300)\nfig.set_facecolor('black')\nax = fig.add_subplot(111, polar=True)\nfor i, color in enumerate(colors):\n    r = np.abs(np.sin((i + 2) * theta))\n    ax.plot(theta, r, color=color, linewidth=2)\nax.set_title('')\nax.grid(False)\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.spines['polar'].set_visible(False)\nax.set_facecolor('black')\nplt.show()\nRunning the above python code creates the following image, we used as our projects logo.\n\n\n\nVT Research Connect Logo\n\n\n\n\nPhotos\n\n\n\nHome Page\n\n\n\n\n\nLogin Page (Not Signed Up Yet)\n\n\n\n\n\nSign Up Page\n\n\n\n\n\nSign Up Confirmation Page\n\n\n\n\n\nConfirmation Email\n\n\n\n\n\nLabs Page\n\n\n\n\n\nAbout Lab Popup\n\n\n\n\n\nDiscussions Page\n\n\n\n\n\nCreate Discussion Popup\n\n\n\n\n\nProfile Page\n\n\n\n\n\nEdit Profile Page"
  },
  {
    "objectID": "posts/projects/jeopardy-java/index.html",
    "href": "posts/projects/jeopardy-java/index.html",
    "title": "Jeopardy - Java",
    "section": "",
    "text": "Overview\nI created a jeopardy game before using Python (Django), but then I learned tools like Jakarta EE and React which I liked much more, so I am continuously working on this project in my free time to recreate a jeopardy application to play with friends. I really enjoy coding like this in my free time.\n\n\nLinks to Repositories\nback-end\nfront-end\n\n\nTech Stack\nBackend: Java (Jakarta EE), Tomcat Server\nDatabase: MySQL\nFrontend: React, TypeScript, Vite\n\n\nPictures\n\n\n\nCreate New Game Page"
  },
  {
    "objectID": "posts/projects/h-l-lyons-books-and-brews/index.html",
    "href": "posts/projects/h-l-lyons-books-and-brews/index.html",
    "title": "H.L. Lyons Books & Brews",
    "section": "",
    "text": "Overview\nI completed this project as part of a Web Application Development course in my Master’s program.\n\n\nLinks to Repositories\nback-end\nfront-end\n\n\nTech Stack\nBackend: Java (Jakarta EE), Tomcat Server\nDatabase: MySQL\nFrontend: React, TypeScript, Vite\n\n\nPhotos\n\n\n\nHome Page\n\n\n\n\n\nCategories Page\n\n\n\n\n\nCart Page\n\n\n\n\n\nCheckout Page\n\n\n\n\n\nConfirmation Page"
  },
  {
    "objectID": "posts/machine-learning-intro/regression/index.html",
    "href": "posts/machine-learning-intro/regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression, a fundamental concept in machine learning, enables us to model relationships between variables. Whether predicting house prices based on square footage or analyzing the impact of advertising spend on sales, regression plays a pivotal role in understanding and predicting patterns in data."
  },
  {
    "objectID": "posts/machine-learning-intro/regression/index.html#introduction",
    "href": "posts/machine-learning-intro/regression/index.html#introduction",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression, a fundamental concept in machine learning, enables us to model relationships between variables. Whether predicting house prices based on square footage or analyzing the impact of advertising spend on sales, regression plays a pivotal role in understanding and predicting patterns in data."
  },
  {
    "objectID": "posts/machine-learning-intro/regression/index.html#data-visualization",
    "href": "posts/machine-learning-intro/regression/index.html#data-visualization",
    "title": "Linear and Nonlinear Regression",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\nI will use the following plot_scatter function to create the data visualizations on this page\n\nimport matplotlib.pyplot as plt\n\ndef plot_scatter(X, X_Range, ax_plot, title):\n  fig, ax = plt.subplots()\n  scatter = ax.scatter(X, y, color=blue, edgecolor='black', alpha=0.6)\n  ax.plot(X_Range, ax_plot, color=purple, linewidth=2)\n  ax.set_title(title, color=purple)\n  ax.set_xlabel('Feature', color=blue)\n  ax.set_ylabel('Target', color=blue)\n  ax.grid(True, linestyle='--', alpha=0.5)\n  ax.tick_params(axis='x', colors=blue)\n  ax.tick_params(axis='y', colors=blue)\n  ax.set_facecolor(pink)\n  fig.set_facecolor(background)\n  plt.show()\n\n\nLinear Regression\nLinear regression is a straightforward method for modeling linear relationships between variables. In a simple linear regression model, the relationship is expressed as \\(y=mx+b\\) where \\(y\\) is the target variable, \\(x\\) is the feature, \\(m\\) is the slope, and \\(b\\) is the intercept. For multiple features, the equation becomes a linear combination.\nLinear regression can be implemented effortlessly using Python’s scikit-learn library. Let’s generate a synthetic dataset and visualize the linear regression line:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Generate synthetic linear data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Train linear regression model\nlinear_reg = LinearRegression()\nlinear_reg.fit(X, y)\n\n# Visualize the linear regression line\nax_plot = linear_reg.predict(X)\nplot_scatter(X, X, ax_plot, 'Linear Regression')\n\n\n\n\n\n\n\n\n\n\nNonlinear Regression\nWhile linear regression is powerful, not all relationships are linear. Nonlinear regression extends the concept by accommodating more complex patterns. Polynomial regression is a common technique, where the relationship between variables is expressed as a polynomial equation.\nLet’s use Python to implement nonlinear regression through polynomial regression. This time, we’ll generate a synthetic dataset with a nonlinear relationship:\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n# Generate synthetic nonlinear data\nnp.random.seed(42)\nX = 6 * np.random.rand(100, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\n# Train nonlinear regression model (polynomial regression)\ndegree = 2\npoly_reg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\npoly_reg.fit(X, y)\n\n# Visualize the nonlinear regression curve\nX_range = np.linspace(-3, 3, 100).reshape(-1, 1)\nax_plot =  poly_reg.predict(X_range)\nplot_scatter(X, X_range, ax_plot, 'Nonlinear Regression (Polynomial)')\n\n\n\n\n\n\n\n\nThe power of regression becomes evident when we visualize the results. Scatter plots with regression lines or curves help us understand how well the model captures the underlying patterns in the data.\nIn conclusion, linear and nonlinear regression are indispensable tools in the machine learning toolbox. While linear regression is effective for simple relationships, nonlinear regression allows us to model more complex patterns. The ability to implement these techniques with Python makes them accessible and applicable to a wide range of real-world scenarios."
  },
  {
    "objectID": "posts/machine-learning-intro/intro-to-ml/index.html",
    "href": "posts/machine-learning-intro/intro-to-ml/index.html",
    "title": "Tetris and Machine Learning",
    "section": "",
    "text": "Little did I know, Tetris, the classic video game, can be related to machine learning and data analytics in various ways.\nI am a huge fan of the game, and I am obsessed with a newer version that facilitates multi-player options; Tetris® Effect: Connected.\n\n\n\n\n\nI decided to stream on twitch a few times… that didn’t go anywhere. There are several other players that I’ve seen stream online, like doremypuyotet, who is a Tetris expert; the video below is them playing.\n[]()\nAs you can see doremypuyotet has incredible technique and it is absolutely mind blowing watching them play the game.\nI wondered though, how would an Artificial Tetris Agent perform at a game of zone battle against someone like doremypuyotet? The agent would have to take several more aspects into consideration than it does in the classic video game. The dynamics of player interactions, adaptive game play, and the wealth of data generated present opportunities to enhance the gaming experience through data-driven insights and algorithmic optimization’s.\n\n\n\n\nAlgorithmic Efficiency: Tetris involves optimizing the placement of falling blocks to complete lines. This is analogous to optimization problems in machine learning where algorithms seek to minimize or maximize a certain objective function.\nHeuristics: Players often develop heuristics or rules of thumb to make decisions quickly. Similarly, in machine learning, heuristics can be used to guide algorithms toward solutions without exhaustive search.\n\n\n\nLearning from Experience: Reinforcement learning involves agents learning by interacting with an environment and receiving feedback in the form of rewards or penalties. In Tetris, players learn from the consequences of their moves, much like reinforcement learning algorithms learn from trial and error.\n\n\n\nFeature Extraction: In Tetris, players recognize patterns in the falling blocks to make decisions. In data analytics, feature extraction involves recognizing patterns in data to enhance the performance of algorithms.\nImage Recognition: Tetris can be framed as an image recognition problem where the goal is to identify patterns in the arrangement of blocks. Image recognition is a common application in machine learning.\n\n\n\nAnticipating Future States: In Tetris, players need to anticipate the future states of the game based on the current configuration of blocks. Predictive analytics in machine learning involves forecasting future trends or outcomes based on historical data.\n\n\n\nReal-time Decision Making: Tetris requires quick decision-making based on the constantly changing game state. In a similar vein, real-time data analytics involves making decisions based on rapidly changing data streams.\n\n\n\nSpatial Visualization: Tetris involves visualizing the spatial arrangement of blocks. Data visualization is a crucial aspect of data analytics, helping individuals understand complex patterns and trends in data through graphical representations.\n\n\n\nGenetic Algorithms: Tetris can be approached as a problem of evolving a strategy over time. Genetic algorithms, a type of evolutionary algorithm, involve evolving solutions to problems through processes inspired by natural selection.\n\n\n\nComplexity Analysis: Analyzing the complexity of Tetris strategies can be similar to analyzing the time and space complexity of algorithms in machine learning. Understanding the efficiency and scalability of algorithms is essential in both contexts.\n\n\n\nManaging Information Overload: Tetris speeds up as the game progresses, leading to an increasing amount of information to process. Dealing with information overload is a challenge in both Tetris and big data analytics.\n\n\n\nGame Theory: Tetris can be viewed through the lens of game theory, where players make strategic decisions to maximize their score. Game theory concepts are also applicable in various areas of machine learning, such as in competitive scenarios."
  },
  {
    "objectID": "posts/machine-learning-intro/intro-to-ml/index.html#tetris-applications-to-machine-learning",
    "href": "posts/machine-learning-intro/intro-to-ml/index.html#tetris-applications-to-machine-learning",
    "title": "Tetris and Machine Learning",
    "section": "",
    "text": "Algorithmic Efficiency: Tetris involves optimizing the placement of falling blocks to complete lines. This is analogous to optimization problems in machine learning where algorithms seek to minimize or maximize a certain objective function.\nHeuristics: Players often develop heuristics or rules of thumb to make decisions quickly. Similarly, in machine learning, heuristics can be used to guide algorithms toward solutions without exhaustive search.\n\n\n\nLearning from Experience: Reinforcement learning involves agents learning by interacting with an environment and receiving feedback in the form of rewards or penalties. In Tetris, players learn from the consequences of their moves, much like reinforcement learning algorithms learn from trial and error.\n\n\n\nFeature Extraction: In Tetris, players recognize patterns in the falling blocks to make decisions. In data analytics, feature extraction involves recognizing patterns in data to enhance the performance of algorithms.\nImage Recognition: Tetris can be framed as an image recognition problem where the goal is to identify patterns in the arrangement of blocks. Image recognition is a common application in machine learning.\n\n\n\nAnticipating Future States: In Tetris, players need to anticipate the future states of the game based on the current configuration of blocks. Predictive analytics in machine learning involves forecasting future trends or outcomes based on historical data.\n\n\n\nReal-time Decision Making: Tetris requires quick decision-making based on the constantly changing game state. In a similar vein, real-time data analytics involves making decisions based on rapidly changing data streams.\n\n\n\nSpatial Visualization: Tetris involves visualizing the spatial arrangement of blocks. Data visualization is a crucial aspect of data analytics, helping individuals understand complex patterns and trends in data through graphical representations.\n\n\n\nGenetic Algorithms: Tetris can be approached as a problem of evolving a strategy over time. Genetic algorithms, a type of evolutionary algorithm, involve evolving solutions to problems through processes inspired by natural selection.\n\n\n\nComplexity Analysis: Analyzing the complexity of Tetris strategies can be similar to analyzing the time and space complexity of algorithms in machine learning. Understanding the efficiency and scalability of algorithms is essential in both contexts.\n\n\n\nManaging Information Overload: Tetris speeds up as the game progresses, leading to an increasing amount of information to process. Dealing with information overload is a challenge in both Tetris and big data analytics.\n\n\n\nGame Theory: Tetris can be viewed through the lens of game theory, where players make strategic decisions to maximize their score. Game theory concepts are also applicable in various areas of machine learning, such as in competitive scenarios."
  },
  {
    "objectID": "posts/machine-learning-intro/classification/index.html",
    "href": "posts/machine-learning-intro/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification, a cornerstone of machine learning, empowers systems to make informed decisions based on input features. From determining whether an email is spam to diagnosing diseases, classification algorithms play a pivotal role in automating decision-making processes.\n\n\nThere are several classification algorithms, each suited to different types of problems:\n\nLogistic Regression: Ideal for binary classification tasks.\nDecision Trees: Effective for both binary and multiclass classification.\nSupport Vector Machines (SVM): Robust for linear and nonlinear classification.\n\nLet’s implement a simple classification model using logistic regression in Python:\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n\n# Generate synthetic classification data\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = (X &gt; 0.5).astype(int).flatten()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_proba = logreg.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "posts/machine-learning-intro/classification/index.html#introduction",
    "href": "posts/machine-learning-intro/classification/index.html#introduction",
    "title": "Classification",
    "section": "",
    "text": "Classification, a cornerstone of machine learning, empowers systems to make informed decisions based on input features. From determining whether an email is spam to diagnosing diseases, classification algorithms play a pivotal role in automating decision-making processes.\n\n\nThere are several classification algorithms, each suited to different types of problems:\n\nLogistic Regression: Ideal for binary classification tasks.\nDecision Trees: Effective for both binary and multiclass classification.\nSupport Vector Machines (SVM): Robust for linear and nonlinear classification.\n\nLet’s implement a simple classification model using logistic regression in Python:\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n\n# Generate synthetic classification data\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = (X &gt; 0.5).astype(int).flatten()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_proba = logreg.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "posts/machine-learning-intro/classification/index.html#data-visualization",
    "href": "posts/machine-learning-intro/classification/index.html#data-visualization",
    "title": "Classification",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\n\nReceiver Operating Characteristic (ROC) Curve\nROC curves visualize the trade-off between true positive rate (sensitivity) and false positive rate. The area under the ROC curve (AUC-ROC) is a valuable metric for model performance.\n\nimport matplotlib.pyplot as plt\n\n# Visualize the ROC curve\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nfig, ax = plt.subplots()\nax.plot(fpr, tpr, color=blue, lw=2, label='ROC curve')\nax.plot([0, 1], [0, 1], color=purple, lw=2, linestyle='--', label='Random Guess')\nax.set_xlabel('False Positive Rate', color=blue)\nax.set_ylabel('True Positive Rate', color=blue)\nax.set_title('Receiver Operating Characteristic (ROC) Curve', color=purple)\nax.legend(loc='lower right')\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPrecision-Recall (PR) Curve\nPR curves focus on the trade-off between precision and recall, particularly valuable in imbalanced datasets.\n\n# Visualize the Precision-Recall curve\nprecision, recall, _ = precision_recall_curve(y_test, y_proba)\nfig, ax = plt.subplots()\nax.plot(recall, precision, color=blue, lw=2, label='Precision-Recall curve')\nax.set_xlabel('Recall (Sensitivity)', color=blue)\nax.set_ylabel('Precision', color=blue)\nax.set_title('Precision-Recall Curve', color=purple)\nax.legend(loc='lower left')\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\nThe confusion matrix provides a detailed understanding of a classification model’s performance, breaking down predictions into true positives, true negatives, false positives, and false negatives.\n\nfrom sklearn.metrics import confusion_matrix\n\n# Generate predictions\ny_pred = logreg.predict(X_test)\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualize the confusion matrix\nfig, ax = plt.subplots()\ncax = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.cool)\nax.set_title('Confusion Matrix', color=purple)\nplt.colorbar(cax)\nclasses = ['Class 0', 'Class 1']\ntick_marks = np.arange(len(classes))\nax.set_xticks(tick_marks)\nax.set_yticks(tick_marks)\nax.set_xticklabels(classes, rotation=45, color=blue)\nax.set_yticklabels(classes, color=blue)\nax.set_xlabel('Predicted label', color=blue)\nax.set_ylabel('True label', color=blue)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\n\n\n\n\nIn conclusion, classification in machine learning is a powerful tool for automating decision-making processes. By implementing and evaluating classification models, we gain insights into their performance through metrics like ROC curves, PR curves, and confusion matrices. These visualizations provide a nuanced understanding of a model’s strengths and weaknesses, facilitating informed decision-making in real-world applications."
  },
  {
    "objectID": "posts/homebrewing/shandy/index.html",
    "href": "posts/homebrewing/shandy/index.html",
    "title": "Hanny’s Shandy",
    "section": "",
    "text": "This was my first homebrew! I did so much research beforehand (mostly on Reddit LOL), but it was such a fun process. I knew this was gonna be the start of a fantastic hobby, and hopefully lead to me owning my own brewery one day!\nI’m just gonna show off the finished product to start this off."
  },
  {
    "objectID": "posts/homebrewing/shandy/index.html#april-12-2024",
    "href": "posts/homebrewing/shandy/index.html#april-12-2024",
    "title": "Hanny’s Shandy",
    "section": "April 12, 2024",
    "text": "April 12, 2024\n\nHeat 2.5 gallons of water. Bring to a boil.\nWhen the water is boiling, remove the kettle from the burner and stir in the 3.15 lbs Pilsen malt syrup.\nReturn wort to boil. The mixture is now called “wort”, the brewer’s term for unfermented beer.\n\nTotal boil time for this recipe is 60 minutes.\nAdd 1 oz Crystal hops at the beginning of the boil.\nAdd the remaining 3 lbs Wheat DME with 15 minutes remaining in the boil.\n\nWhen the 60-minute boil is finished, cool the wort to approximately 100° F as rapidly as possible. I put the kettle in a large tub filled with ice, you can also put the kettle in an ice bath in your kitchen sink.\nWhile the wort cools, sanitize the fermenting equipment – fermenter, lid or stopper, airlock, funnel, etc – along with the yeast packet.\nFill the primary fermenter with 2 gallons of cold water (I had my primary fermenter sitting in a cooler with ice, to get the 2 gallons of water cold, while I was waiting for the wort to cool), then pour in the cooled wort. Leave any thick sludge in the bottom of the kettle.\n\n\n\nAdd more cold water as needed to bring the volume to 5 gallons.\nSeal the fermenter and rock back and forth to splash for a few minutes to aerate the wort.\nMeasure specific gravity of the wort with a hydrometer and record this number. I kept mine in the notes section of my phone, along with the date.\nAdd yeast once the temperature of the wort is 75°F or lower (not warm to the touch). Sanitize and open the yeast pack and carefully pour the contents into the primary fermenter.\nSeal the fermenter. Add approximately 1 tablespoon of water to the sanitized fermentation lock. Insert the airlock into rubber stopper or lid and seal the fermenter.\nMove the fermenter to a warm, dark, quiet spot until fermentation begins."
  },
  {
    "objectID": "posts/homebrewing/shandy/index.html#april-12-2024---april-26-2024",
    "href": "posts/homebrewing/shandy/index.html#april-12-2024---april-26-2024",
    "title": "Hanny’s Shandy",
    "section": "April 12, 2024 - April 26, 2024",
    "text": "April 12, 2024 - April 26, 2024\n\nWithin approximately 48 hours of Brewing Day, active fermentation will begin. The optimum fermentation temperature for this beer is 65°- 70°F. Move the fermenter to a warmer or cooler spot as needed.\nApproximately one to two weeks after brewing day, active fermentation will end.\nI measured the specific gravity on day 12 (April 24, 2024) and day 14 (April 26, 2024) and it was steady. My beer ended up being approximately 5% ABV. I unfortunately don’t have a picture of the original gravity reading, but below is a picture of the final gravity reading.\n\n\n\nI decided to go ahead and bottle my beers on day 14. I probably should have waited a bit longer, but I was really eager to try the beer 😆."
  },
  {
    "objectID": "posts/homebrewing/shandy/index.html#april-26-2024",
    "href": "posts/homebrewing/shandy/index.html#april-26-2024",
    "title": "Hanny’s Shandy",
    "section": "April 26, 2024",
    "text": "April 26, 2024\n\nSanitize siphoning and bottling equipment.\nMix a priming solution; I used Corn sugar (dextrose), 2/3 cup in 16 oz water. Bring this solution to a boil, stir in the 12 grams of Crystallized Lemon and pour into the bottling bucket.\nSiphon beer into bottling bucket and mix with priming solution. Stir gently to mix—don’t splash.\nFill and cap bottles.\n\n\n\nCondition bottles at room temperature for 1–2 weeks. After this point, the bottles can be stored cool or cold."
  },
  {
    "objectID": "posts/homebrewing/shandy/index.html#may-2-2024",
    "href": "posts/homebrewing/shandy/index.html#may-2-2024",
    "title": "Hanny’s Shandy",
    "section": "May 2, 2024",
    "text": "May 2, 2024\nI was a little impatience on waiting the full 1-2 weeks, so I chilled one of the bottles and tried it on day 6 of bottle conditioning (but I let the rest continue conditioning).\n\n\n\n\n\nThe beer was SO good. I was so pleasantly surprised. It was bubbly, cold, and crisp. I was so excited to do another batch."
  },
  {
    "objectID": "posts/homebrewing/shandy/index.html#may-8-2024",
    "href": "posts/homebrewing/shandy/index.html#may-8-2024",
    "title": "Hanny’s Shandy",
    "section": "May 8, 2024",
    "text": "May 8, 2024\nFinally, after bottling conditioning was complete, I labeled all the bottles. When I started dating my boyfriend (Liam) back in 2020, I told him I wanted to have my own brewery one day. He doodled up an amazing logo.\n\n\n\n\n\nFour years later, he recreated it using Adobe Illustrator, and he conveniently works at a print shop and was able to print the labels. My best friend (Patty) named the beer, Hanny’s Shandy. Liam then created a label specifically for the beer. I LOVE IT. One day you’ll see these all over the place (I hope)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a Software Engineer working for Estes Express Lines. I work on an agile development team creating and improving features for software applications. I am super passionate about front and back end web development (full stack), as well as general software development for desktop applications, etc. Please check out the Blog page to see coding projects from both my personal and academic career."
  },
  {
    "objectID": "index.html#my-academic-career-timeline",
    "href": "index.html#my-academic-career-timeline",
    "title": "About",
    "section": "My Academic & Career Timeline",
    "text": "My Academic & Career Timeline\n\n\n\nDescending\n\n\n\n\n\nJuly 2025\n\nSoftware Engineer II at Estes Express Lines\n\n\nSoftware engineer on an agile development team creating and improving features for software applications.\n\n\n\n\n\nSeptember 2024 - May 2025\n\nSoftware Developer at Naval Surface Warfare Center Dahlgren Division\n\n\nSoftware developer on an agile development team creating and improving features for a modeling and simulation software application. Other responsibilities include leading code reviews, mentoring junior developers, and contributing to technical design decisions.\n\n\n\n\n\nAugust 2024\n\nConferred the Master of Engineering\n\n\nConferred the Master of Engineering. Major – Computer Science and Applications on August 9th, 2024. Capstone Project showcased on VTechWorks.\n\n\n\n\n\nAugust 2023\n\nGraduate Student at Virginia Tech Innovation Campus\n\n\nI began as a graduate student at Virginia Tech. I was awarded the Boeing Graduate Scholarship which was highlighted on Virginia Tech News.\n\n\n\n\n\nJune 2021 - September 2024\n\nSoftware Developer at Naval Surface Warfare Center Indian Head Division\n\n\nI worked as the technical lead software developer on the Modeling and Simulations (M&S) Team. I worked with a team of developers on the creation of a Java-based software application over a multi-year period. Led the project management for several high–intensity, short duration funded efforts, delivering M&S support as requested by upper-level management.\n\n\n\n\n\nMay 2021\n\nConferred the Bachelor of Science\n\n\nConferred the Bachelor of Science. Major – Biochemistry. Magna Cum Laude, on May 15, 2021. Completed the Minor in Chemistry. Completed the Minor in Computer Science.\n\n\n\n\n\nJune 2020 - August 2020 & December 2020 - January 2021\n\nStudent Intern at Naval Surface Warfare Center Indian Head Division\n\n\nStarted as a student intern working with the Modeling & Simulations Team for Chemical, Biological, and Radiological (CBR) Defense. During my time as an intern, I conducted a comprehensive literature review and addressed foundational questions contributing to the successful initiation of their newly funded 3-year software development efforts.\n\n\n\n\n\nAugust 2019\n\nComputer Science Minor\n\n\nI really liked biochemistry as a major (mostly just the chemistry classes), but I still wanted to broaden my knowledge! I realized I had enough open electives left to add a minor, and computer science was calling to me. The first class I ever took was an insane learning curve, but also an incredibly rewarding challenge. I knew after successfully completing that class, to have a career in the field of computer science would be beyond gratifying.\n\n\n\n\n\nMay 2019 - August 2019\n\nBookkeeper at Koons Chevy Buick GMC\n\n\nI worked in the accounting department as a bookkeeper. My daily tasks included things like preparing and handling legal data for vehicle registration and taxation. Lot’s of paper work! The car industry loves paper!\n\n\n\n\n\nAugust 2017\n\nUndergraduate Student at Virginia Tech\n\n\nI began as an undergraduate student at Virgina Tech. Many major changes later until I decided on Biochemestry! General Chemisty was my favorite class!\n\n\n\n\n\nJune 2017\n\nGraduated High School\n\n\nHappy to be done! I worked the rest of the summer to make money before starting college in August.\n\n\n\n\n\nMarch 2016 - August 2017\n\nStarted my First Job!\n\n\nStarted as an Associate at Potbelly Sandwich Works. Quickly promoted to be a certified trainer, training all new employees. Learned valuable teamwork and customer service skills."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Series",
    "section": "",
    "text": "Series\ncoding-projects\nThis series contains a great deal of coding projects completed in both my academic and personal life.\n\n\nAll Blog Posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nJeopardy - Java\n\n\n\n\n\n\nCode\n\n\nPersonal\n\n\n\nJava (Jakarta EE), Tomcat Server, MySQL, React, Typescript, Vite\n\n\n\n\n\nJan 1, 2025\n\n\n1 min\n\n\n\n\n\n\n\nVT Research Connect\n\n\n\n\n\n\nCode\n\n\nAcademic\n\n\n\nJava (Spring Boot), PostgreSQL, React, Typescript, Vite\n\n\n\n\n\nAug 6, 2024\n\n\n2 min\n\n\n\n\n\n\n\nH.L. Lyons Books & Brews\n\n\n\n\n\n\nCode\n\n\nAcademic\n\n\n\nJava (Jakarta EE), Tomcat Server, MySQL, React, Typescript, Vite\n\n\n\n\n\nAug 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nKeezer Build\n\n\n\n\n\n\nDIY\n\n\nHomebrewing\n\n\n\nDetailed instructions on how I built my Keezer!\n\n\n\n\n\nMay 31, 2024\n\n\n6 min\n\n\n\n\n\n\n\nHokie Housing Hub\n\n\n\n\n\n\nCode\n\n\nAcademic\n\n\n\nHTML, CSS, JavaScript\n\n\n\n\n\nApr 14, 2024\n\n\n1 min\n\n\n\n\n\n\n\nHanny’s Shandy\n\n\n\n\n\n\nHomebrewing\n\n\n\nLemon Summer Shandy Homebrew Recipe\n\n\n\n\n\nApr 12, 2024\n\n\n6 min\n\n\n\n\n\n\n\nJeopardy - Python\n\n\n\n\n\n\nCode\n\n\nPersonal\n\n\n\nPython (Django), PostgreSQL, HTML, CSS, JavaScript\n\n\n\n\n\nFeb 15, 2024\n\n\n1 min\n\n\n\n\n\n\n\nFocal\n\n\n\n\n\n\nCode\n\n\nAcademic\n\n\n\nPython (Django), PostgreSQL, HTML, CSS, JavaScript\n\n\n\n\n\nDec 1, 2023\n\n\n1 min\n\n\n\n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\nCode\n\n\nAcademic\n\n\n\nUnveiling Anomalies with Outlier Detection in Machine Learning\n\n\n\n\n\nNov 20, 2023\n\n\n2 min\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\nCode\n\n\nAcademic\n\n\n\nROC, PR, Confusion Matrix\n\n\n\n\n\nOct 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\nCode\n\n\nAcademic\n\n\n\nNavigating Trends with Linear and Nonlinear Regression in Machine Learning\n\n\n\n\n\nOct 20, 2023\n\n\n3 min\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\nCode\n\n\nAcademic\n\n\n\nUnraveling Patterns with Clustering in Machine Learning\n\n\n\n\n\nSep 30, 2023\n\n\n2 min\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\nCode\n\n\nAcademic\n\n\n\nUnderstanding Probability Theory and Random Variables in the Context of Machine Learning\n\n\n\n\n\nSep 20, 2023\n\n\n4 min\n\n\n\n\n\n\n\nTetris and Machine Learning\n\n\n\n\n\n\nCode\n\n\nAcademic\n\n\n\nMachine Learning and its applications to the classic videogame: Tetris\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Jeopardy - Java\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\n\n\n\n\n\nVT Research Connect\n\n\n\n\n\n\n\n\nAug 6, 2024\n\n\n\n\n\n\n\nH.L. Lyons Books & Brews\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\n\n\n\n\n\nKeezer Build\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\n\nHokie Housing Hub\n\n\n\n\n\n\n\n\nApr 14, 2024\n\n\n\n\n\n\n\nHanny’s Shandy\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\nJeopardy - Python\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\n\n\n\n\n\nFocal\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\nSep 30, 2023\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\n\n\n\n\n\nTetris and Machine Learning\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "code-projects-series.html",
    "href": "code-projects-series.html",
    "title": "Series: Coding Projects",
    "section": "",
    "text": "Jeopardy - Java\n\n\n\n\n\nJava (Jakarta EE), Tomcat Server, MySQL, React, Typescript, Vite\n\n\n\n\n\nJan 1, 2025\n\n\n1 min\n\n\n\n\n\n\n\nVT Research Connect\n\n\n\n\n\nJava (Spring Boot), PostgreSQL, React, Typescript, Vite\n\n\n\n\n\nAug 6, 2024\n\n\n2 min\n\n\n\n\n\n\n\nH.L. Lyons Books & Brews\n\n\n\n\n\nJava (Jakarta EE), Tomcat Server, MySQL, React, Typescript, Vite\n\n\n\n\n\nAug 1, 2024\n\n\n1 min\n\n\n\n\n\n\n\nHokie Housing Hub\n\n\n\n\n\nHTML, CSS, JavaScript\n\n\n\n\n\nApr 14, 2024\n\n\n1 min\n\n\n\n\n\n\n\nJeopardy - Python\n\n\n\n\n\nPython (Django), PostgreSQL, HTML, CSS, JavaScript\n\n\n\n\n\nFeb 15, 2024\n\n\n1 min\n\n\n\n\n\n\n\nFocal\n\n\n\n\n\nPython (Django), PostgreSQL, HTML, CSS, JavaScript\n\n\n\n\n\nDec 1, 2023\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/diy/keezer/index.html",
    "href": "posts/diy/keezer/index.html",
    "title": "Keezer Build",
    "section": "",
    "text": "I just gotta show her off"
  },
  {
    "objectID": "posts/diy/keezer/index.html#painting-the-chest-freezer",
    "href": "posts/diy/keezer/index.html#painting-the-chest-freezer",
    "title": "Keezer Build",
    "section": "Painting the Chest Freezer",
    "text": "Painting the Chest Freezer\nPainting the chest freezer was definitely the easiest part. First I sanded the whole thing and put painters tape around the seal, drain hole, and the little green light that shows the freezer is on.\n\n\n\n\n\nThen I primed the freezer, I didn’t prime the top because I wouldn’t be painting that. I knew I was going to be adding the tabletop later on.\n\n\n\n\n\nA day or two later I pained it this lovely blue color. The lid is also now removed because I was prepping for the tabletop to be added."
  },
  {
    "objectID": "posts/diy/keezer/index.html#connecting-the-wooden-tabletop-and-the-freezer-lid",
    "href": "posts/diy/keezer/index.html#connecting-the-wooden-tabletop-and-the-freezer-lid",
    "title": "Keezer Build",
    "section": "Connecting the Wooden Tabletop and the Freezer Lid",
    "text": "Connecting the Wooden Tabletop and the Freezer Lid\nThe first thing I had to do was drill a hole in the freezer lid, I used a hole saw to do this. Unfortunately, I drilled right through the wires that connected to the freezer light in the lid, but I carefully removed them so it wasn’t an issue. And I didn’t need the light anyways so I wasn’t upset about it.\n\n\n\n\n\nNext I had to line up the hole in the freezer to the wooden piece. This was easy. I also added painters tape to show where the freezer lid will go.\n\n\n\n\n\nHere I laid out the 1/2” washers to go in between the wood and the freezer lid. This was because the lid had the smallest dip on the edges raising it, and I needed this lid to be as secure as possible to the wood.\n\n\n\n\n\nFinally here’s everything glued on to the wood and ready to have to freezer lid placed on top.\n\n\n\n\n\nHere’s the lid glued on and with additional washers that I screwed through the freezer lid into the wood. I let this set for at least 48 hours before messing with it."
  },
  {
    "objectID": "posts/diy/keezer/index.html#finishing-the-wooden-tabletop",
    "href": "posts/diy/keezer/index.html#finishing-the-wooden-tabletop",
    "title": "Keezer Build",
    "section": "Finishing the Wooden Tabletop",
    "text": "Finishing the Wooden Tabletop\nI had to add tabletop edges to this thing. Let me tell you, using a table saw to cut 45 degree angled edges was not easy! And table saws are actually terrifying. Here I have the three pieces of wood ready to to be attached and make this baby look more elegant.\n\n\n\n\n\nSo I am silly and only bought gray glue. I should have gotten one that matched the stain I was going to use, but I masking taped this baby up to help prevent any gray glue getting anywhere. I needed several hands for this, I was going to glue the edges on, and then also nail them in with really tiny nails.\n\n\n\n\n\nHere she is with the edges 😍 This was definitely the most stressful part of this build. It didn’t take long but I was so worried about the gluing and nailing and not having the edges aligned perfectly, but thankfully it all worked out!\n\n\n\n\n\nThere was some issues, I didn’t have any really big clamps to push the wood super close together, so there are gaps between the top and the edges, and some places you could see glue. I ended up picking out the gray glue later on and filling those edges with red oak wood filler before I stained. This helped A LOT. If I were to redo this, I would definitely get big clamps so those gaps wouldn’t be there.\n\n\n\n\n\nI also filled all the nails in with red oak wood filler before I stained as well."
  },
  {
    "objectID": "posts/diy/keezer/index.html#staining-the-tabletop",
    "href": "posts/diy/keezer/index.html#staining-the-tabletop",
    "title": "Keezer Build",
    "section": "Staining the Tabletop",
    "text": "Staining the Tabletop\nI obviously sanded the crap out of this before I stained. I LOVED how this stain turned out. I love the grain pattern for red oak wood, it is a pricier wood but it is just so beautiful.\n\n\n\n\n\nHere is after the second stain.\n\n\n\n\n\nI did some final touch ups with the red oak wood filler, did one more stain, and finished with polyurethane."
  },
  {
    "objectID": "posts/diy/keezer/index.html#making-the-logo",
    "href": "posts/diy/keezer/index.html#making-the-logo",
    "title": "Keezer Build",
    "section": "Making the Logo",
    "text": "Making the Logo\nThe logo is probably my favorite part. When I started dating my boyfriend (Liam) back in 2020, I told him I wanted to have my own brewery one day. He doodled up an amazing logo.\n\n\n\n\n\nFour years later, he recreated it using Adobe Illustrator, and he conveniently works at a print shop and printed the logo directly on to aluminum dibond."
  },
  {
    "objectID": "posts/diy/keezer/index.html#finishing-touches",
    "href": "posts/diy/keezer/index.html#finishing-touches",
    "title": "Keezer Build",
    "section": "Finishing Touches",
    "text": "Finishing Touches\nFinally after the wood was done drying (I waited 1 week) I attached the tap tower. I then glued small wooden pieces to the front of the freezer. Then I took the circular stained piece of wood (Liam got everything cut into circles at his job, I didn’t have a tool for that) and glued that onto the smaller pieces of wood to make it look elevated off the wood. I also know you’re really not supposed to have anything covering a freezer, because it needs to release heat, so I didn’t want the big circular piece of wood directly glued onto the freezer. Then I just glued the logo onto the piece of wood!\n\n\n\n\n\nFinally I added LED lights underneath the lid to illuminate this masterpiece at night!\n\n\n\n\n\nThen I added all the keg elements into the freezer, along with attaching the Inkbird temperature controller to keep the freezer at usually around 34 degrees, I like a crisp cold beer 😎\n\n\n\n\n\nAnd finally, at night 😏"
  },
  {
    "objectID": "posts/machine-learning-intro/anomaly/index.html",
    "href": "posts/machine-learning-intro/anomaly/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection, a critical aspect of machine learning, involves identifying patterns in data that deviate significantly from the norm. This technique finds applications in various domains, from fraud detection in financial transactions to detecting anomalies in network traffic for cybersecurity.\n\n\nWhile anomaly detection is a powerful tool, it comes with challenges. Real-world data often exhibits diverse distributions, and datasets can be imbalanced, making it tricky to identify rare anomalies accurately. Robust algorithms are essential to overcome these challenges."
  },
  {
    "objectID": "posts/machine-learning-intro/anomaly/index.html#introduction",
    "href": "posts/machine-learning-intro/anomaly/index.html#introduction",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection, a critical aspect of machine learning, involves identifying patterns in data that deviate significantly from the norm. This technique finds applications in various domains, from fraud detection in financial transactions to detecting anomalies in network traffic for cybersecurity.\n\n\nWhile anomaly detection is a powerful tool, it comes with challenges. Real-world data often exhibits diverse distributions, and datasets can be imbalanced, making it tricky to identify rare anomalies accurately. Robust algorithms are essential to overcome these challenges."
  },
  {
    "objectID": "posts/machine-learning-intro/anomaly/index.html#data-visualization",
    "href": "posts/machine-learning-intro/anomaly/index.html#data-visualization",
    "title": "Anomaly/Outlier Detection",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\n\nDBSCAN and Evaluation Metrics for Anomaly Detection\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) emerges as a robust algorithm for anomaly detection. Unlike traditional methods, DBSCAN doesn’t assume a specific shape for clusters and can effectively isolate points in low-density regions as outliers.\nVisualizing anomalies is crucial for interpreting model results. Scatter plots with DBSCAN labels provide an intuitive representation of the identified anomalies in the dataset.\nEvaluating anomaly detection models poses challenges due to the scarcity of anomalies. Metrics such as precision, recall, and F1 score provide a nuanced understanding of the model’s performance.\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to generate a synthetic dataset with outliers\ndef generate_dataset_with_outliers(seed=42):\n    np.random.seed(seed)\n    X, _ = make_blobs(n_samples=300, centers=1, cluster_std=1, random_state=0)\n    outliers = np.random.uniform(low=-10, high=10, size=(20, 2))\n    return np.concatenate([X, outliers]), np.concatenate([np.zeros(300), -np.ones(20)])\n\n# Step 1: Generate a synthetic dataset with outliers\nX, y_true = generate_dataset_with_outliers()\n\n# Step 2: Train a DBSCAN model for anomaly detection\ndbscan = DBSCAN(eps=1, min_samples=5)\npredicted_labels = dbscan.fit_predict(X)\n\n# Step 3: Visualize anomalies with DBSCAN labels\nfig, ax = plt.subplots()\nscatter = ax.scatter(X[:, 0], X[:, 1], c=predicted_labels, cmap=plt.cm.cool, edgecolor='black', s=50)\nax.set_title('Anomaly Detection with DBSCAN', color=purple)\nax.set_xlabel('Feature 1', color=blue)\nax.set_ylabel('Feature 2', color=blue)\nax.grid(True, linestyle='--', alpha=0.5)\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n# Step 4: Evaluate the model using precision, recall, and F1 score\nprecision = precision_score(y_true, predicted_labels, pos_label=-1)\nrecall = recall_score(y_true, predicted_labels, pos_label=-1)\nf1 = f1_score(y_true, predicted_labels, pos_label=-1)\n\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\n\n\n\n\n\n\n\nPrecision: 1.00\nRecall: 0.90\nF1 Score: 0.95\n\n\nAnomaly detection, powered by algorithms like DBSCAN, plays a vital role in uncovering irregularities in data. The combination of effective visualizations and evaluation metrics enables us to gain insights into the anomalies present, providing a foundation for critical decision-making in various domains."
  },
  {
    "objectID": "posts/machine-learning-intro/clustering/index.html",
    "href": "posts/machine-learning-intro/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering, a fundamental technique in machine learning, is the process of grouping similar data points together. Its applications are diverse, ranging from customer segmentation in marketing to anomaly detection in cybersecurity. By identifying patterns within datasets, clustering enables us to gain valuable insights and make informed decisions.\n\n\nClustering algorithms come in various flavors, each with its unique approach to grouping data:\n\nK-Means: Divides data into k clusters, where each cluster is represented by its centroid.\nHierarchical Clustering: Forms a tree of clusters, allowing for hierarchical organization.\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): A density-based algorithm that identifies clusters based on the density of data points.\n\nDBSCAN stands out for its ability to discover clusters of arbitrary shapes. The algorithm works by identifying core points, which are densely packed, and expanding clusters by connecting core points. It also identifies noise points that do not belong to any cluster."
  },
  {
    "objectID": "posts/machine-learning-intro/clustering/index.html#introduction",
    "href": "posts/machine-learning-intro/clustering/index.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "Clustering, a fundamental technique in machine learning, is the process of grouping similar data points together. Its applications are diverse, ranging from customer segmentation in marketing to anomaly detection in cybersecurity. By identifying patterns within datasets, clustering enables us to gain valuable insights and make informed decisions.\n\n\nClustering algorithms come in various flavors, each with its unique approach to grouping data:\n\nK-Means: Divides data into k clusters, where each cluster is represented by its centroid.\nHierarchical Clustering: Forms a tree of clusters, allowing for hierarchical organization.\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): A density-based algorithm that identifies clusters based on the density of data points.\n\nDBSCAN stands out for its ability to discover clusters of arbitrary shapes. The algorithm works by identifying core points, which are densely packed, and expanding clusters by connecting core points. It also identifies noise points that do not belong to any cluster."
  },
  {
    "objectID": "posts/machine-learning-intro/clustering/index.html#data-visualization",
    "href": "posts/machine-learning-intro/clustering/index.html#data-visualization",
    "title": "Clustering",
    "section": "Data Visualization",
    "text": "Data Visualization\nLet’s demonstrate the power of DBSCAN using Python and scikit-learn. We’ll start by generating a synthetic dataset:\n\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# Generate a synthetic dataset\nX, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\nVisualizing the results of clustering is crucial for understanding the underlying structure of the data. Let’s create a scatter plot that represents the original data points with cluster labels:\n\nimport matplotlib.pyplot as plt\n\n# Visualize the clustering\nfig, ax = plt.subplots()\nscatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap=plt.cm.cool, edgecolor='black', s=50)\nax.set_title('DBSCAN Clustering', color=purple)\nax.set_xlabel('Feature 1', color=blue)\nax.set_ylabel('Feature 2', color=blue)\nax.grid(True, linestyle='--', alpha=0.5)\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\n\n\n\n\nClustering, especially with the powerful DBSCAN algorithm, opens new avenues for extracting patterns from data. As we’ve seen, DBSCAN’s ability to identify clusters based on density makes it versatile for a wide range of applications. By visualizing the results, we gain a deeper understanding of the data’s inherent structure, providing a solid foundation for subsequent analysis and decision-making."
  },
  {
    "objectID": "posts/machine-learning-intro/probability/index.html",
    "href": "posts/machine-learning-intro/probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In the realm of machine learning, uncertainty is inherent. Models make predictions based on patterns learned from data, and probability theory provides a formal framework for dealing with uncertainty. Random variables are employed to represent uncertain quantities, making probability theory a cornerstone for building and interpreting machine learning models.\nMachine learning algorithms often assume specific probability distributions. For instance:\n\nThe normal distribution is frequently assumed in linear regression and Gaussian processes.\nThe binomial distribution is relevant for problems involving binary outcomes, such as classification tasks.\n\nUnderstanding the characteristics of these distributions is vital when selecting appropriate models and interpreting results.\nRandom variables represent various aspects of data. Features can be modeled as random variables, and understanding their distribution is crucial for feature engineering. Additionally, the uncertainty associated with predictions is often expressed through probability distributions, making random variables central to the prediction process."
  },
  {
    "objectID": "posts/machine-learning-intro/probability/index.html#introduction",
    "href": "posts/machine-learning-intro/probability/index.html#introduction",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In the realm of machine learning, uncertainty is inherent. Models make predictions based on patterns learned from data, and probability theory provides a formal framework for dealing with uncertainty. Random variables are employed to represent uncertain quantities, making probability theory a cornerstone for building and interpreting machine learning models.\nMachine learning algorithms often assume specific probability distributions. For instance:\n\nThe normal distribution is frequently assumed in linear regression and Gaussian processes.\nThe binomial distribution is relevant for problems involving binary outcomes, such as classification tasks.\n\nUnderstanding the characteristics of these distributions is vital when selecting appropriate models and interpreting results.\nRandom variables represent various aspects of data. Features can be modeled as random variables, and understanding their distribution is crucial for feature engineering. Additionally, the uncertainty associated with predictions is often expressed through probability distributions, making random variables central to the prediction process."
  },
  {
    "objectID": "posts/machine-learning-intro/probability/index.html#data-visualization",
    "href": "posts/machine-learning-intro/probability/index.html#data-visualization",
    "title": "Probability Theory and Random Variables",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\nI will use the following plot_histograms function to create the data visualizations on this page\n\nimport matplotlib.pyplot as plt\n\n# Function to plot histograms\ndef plot_histograms(data, title):\n    # Create a histogram\n    fig, ax = plt.subplots()\n    hist = ax.hist(data, bins=30, edgecolor='black', alpha=0.7, color=purple, ec=blue)\n    ax.set_title(title, color=purple)\n    ax.set_xlabel('Values', color=blue)\n    ax.set_ylabel('Frequency', color=blue)\n    ax.grid(True, linestyle='--', alpha=0.5)\n    ax.tick_params(axis='x', colors=blue)\n    ax.tick_params(axis='y', colors=blue)\n    \n    ax.set_facecolor(pink)\n    fig.set_facecolor(background)\n    \n    # Show the plot\n    plt.show()\n\nLet’s use Python to create a histogram of a simulated random variable:\n\nimport numpy as np\n\n# Sets the seed for the random number generator to 42\nnp.random.seed(42)\n# Generates an array of 1000 random numbers drawn from a normal distribution\n# using the random number generated defined above\ndata = np.random.normal(size=1000)\nplot_histograms(data, 'Histogram of Random Variable')\n\n\n\n\n\n\n\n\nProbability theory and random variables are not just theoretical concepts; they are integral to the practical application of machine learning. Models are built upon the assumptions of probability distributions, and the visualization of data through histograms aids in model interpretation and validation. As you dive deeper into machine learning, a solid understanding of these foundational concepts will empower you to make more informed decisions in model selection, training, and evaluation.\nThe following code generates a dataset with two classes, trains a Random Forest Classifier on the data, and then generates predictions on a test set. The histograms before and after applying the machine learning model are plotted on top of one another for comparison.\nThe generate_dataset function creates a synthetic dataset with two classes, and the plot_histograms function is used to visualize the distribution before and after applying the machine learning model. The accuracy of the model is also printed to give an idea of its performance.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Function to generate a random dataset with two classes\ndef generate_dataset(seed=42):\n    np.random.seed(seed)\n    class1 = np.random.normal(loc=0, scale=1, size=500)\n    class2 = np.random.normal(loc=3, scale=1, size=500)\n    labels = [0] * 500 + [1] * 500\n    data = np.concatenate([class1, class2])\n    return data, labels\n\n# Step 1: Generate a random dataset\ndata_before, labels = generate_dataset()\n\n# Step 2: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data_before.reshape(-1, 1), labels, test_size=0.2, random_state=42)\n\n# Step 3: Train a Random Forest Classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Step 4: Make predictions on the test set\npredictions = clf.predict(X_test)\n\n# Step 5: Assess accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy of the model: {accuracy:.2%}\")\n\n# Step 6: Generate a dataset after applying the machine learning model\ndata_after = np.concatenate([clf.predict_proba(X_test)[:, 0], clf.predict_proba(X_test)[:, 1]])\n\n# Step 7: Plot histograms to compare before and after\nplot_histograms(data_before, 'Original Dataset')\nplot_histograms(data_after, 'Dataset after Machine Learning')\n\nAccuracy of the model: 89.00%"
  },
  {
    "objectID": "posts/projects/focal/index.html",
    "href": "posts/projects/focal/index.html",
    "title": "Focal",
    "section": "",
    "text": "Overview\nI completed this project as part of a User Interface Software course in my Master’s program.\n\n\nLinks to Repositories\ncode\n\n\nWebsite Link\nAfter completing the initial HTML, CSS, JavaScript for the project, we learned how to upload our information to just a file system hosted by VT, so this link is purely a static site. Throughout the course we used Django to create a dynamic site, that is not hosted anywhere but the repository link has the most up to date code.\n\n\nTech Stack\nBackend: Python (Django)\nDatabase: PostgreSQL\nFrontend: HTML, CSS, JavaScript\n\n\nPhotos\n\n\n\nHome Page\n\n\n\n\n\nReviews Page\n\n\n\n\n\nAdd Review Page\n\n\n\n\n\nReview Comments Page\n\n\n\n\n\nEdit Profile Page"
  },
  {
    "objectID": "posts/projects/hokie-housing-hub/index.html",
    "href": "posts/projects/hokie-housing-hub/index.html",
    "title": "Hokie Housing Hub",
    "section": "",
    "text": "Overview\nI completed this project as part of a Project Management course in my Master’s program. I uploaded the files to the file system hosted by VT, so this link is purely a static site.\n\n\nLinks to Repositories\ncode\n\n\nTech Stack\nHTML, CSS, JavaScript\n\n\nPhotos\n\n\n\nHome Page\n\n\n\n\n\nLogin Page\n\n\n\n\n\nOn Campus Page\n\n\n\n\n\nReviews Page\n\n\n\n\n\nAdd Review Page"
  },
  {
    "objectID": "posts/projects/jeopardy-python/index.html",
    "href": "posts/projects/jeopardy-python/index.html",
    "title": "Jeopardy - Python",
    "section": "",
    "text": "Overview\nI created this project for fun because my best friend was finally moving to the same city as me, and I hosted her and her boyfriend a welcome party! I created this website in just two weeks, and I am proud of it for having only learned Django in school one semester before. I haven’t touched this project since then, but it’s definitely a fun little personalized game (I kind of turned it into a drinking game LOL). Everyone enjoyed it, I love being able to do small things like this with my coding abilities!\n\n\nLinks to Repositories\ncode\n\n\nTech Stack\nBackend: Python (Django)\nDatabase: PostgreSQL\nFrontend: HTML, CSS, JavaScript\n\n\nPhotos\n\n\n\nHome Page\n\n\n\n\n\nJeopardy Board Page\n\n\n\n\n\nQuestion Page"
  }
]