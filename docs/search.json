[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In the realm of machine learning, uncertainty is inherent. Models make predictions based on patterns learned from data, and probability theory provides a formal framework for dealing with uncertainty. Random variables are employed to represent uncertain quantities, making probability theory a cornerstone for building and interpreting machine learning models."
  },
  {
    "objectID": "posts/probability/index.html#introduction",
    "href": "posts/probability/index.html#introduction",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In the realm of machine learning, uncertainty is inherent. Models make predictions based on patterns learned from data, and probability theory provides a formal framework for dealing with uncertainty. Random variables are employed to represent uncertain quantities, making probability theory a cornerstone for building and interpreting machine learning models."
  },
  {
    "objectID": "posts/probability/index.html#probability-distributions",
    "href": "posts/probability/index.html#probability-distributions",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nMachine learning algorithms often assume specific probability distributions. For instance:\n\nThe normal distribution is frequently assumed in linear regression and Gaussian processes.\nThe binomial distribution is relevant for problems involving binary outcomes, such as classification tasks.\n\nUnderstanding the characteristics of these distributions is vital when selecting appropriate models and interpreting results."
  },
  {
    "objectID": "posts/probability/index.html#random-variables",
    "href": "posts/probability/index.html#random-variables",
    "title": "Probability Theory and Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nIn machine learning, random variables represent various aspects of data. Features can be modeled as random variables, and understanding their distribution is crucial for feature engineering. Additionally, the uncertainty associated with predictions is often expressed through probability distributions, making random variables central to the prediction process."
  },
  {
    "objectID": "posts/probability/index.html#data-visualization",
    "href": "posts/probability/index.html#data-visualization",
    "title": "Probability Theory and Random Variables",
    "section": "Data Visualization",
    "text": "Data Visualization\nData visualization, including histograms, plays a crucial role in the exploratory data analysis phase of machine learning projects. Understanding the distribution of features and target variables helps practitioners make informed decisions about preprocessing steps and the selection of appropriate algorithms.\nThe Python code provided for generating histograms is a practical example of how data visualization is implemented in machine learning. Visualizing the distribution of data is a critical step in understanding its characteristics before applying machine learning algorithms.\nLet’s use Python and Matplotlib to create a histogram of a simulated random variable:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate a random dataset (replace this with your own data)\nnp.random.seed(42)\ndata = np.random.normal(size=1000)\n\n# Create a histogram\nplt.hist(data, bins=30, edgecolor='black', alpha=0.7)\nplt.title('Histogram of Random Variable')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n\n\n\n\nProbability theory and random variables are not just theoretical concepts; they are integral to the practical application of machine learning. Models are built upon the assumptions of probability distributions, and the visualization of data through histograms aids in model interpretation and validation. As you dive deeper into machine learning, a solid understanding of these foundational concepts will empower you to make more informed decisions in model selection, training, and evaluation.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Function to generate a random dataset with two classes\ndef generate_dataset(seed=42):\n    np.random.seed(seed)\n    class1 = np.random.normal(loc=0, scale=1, size=500)\n    class2 = np.random.normal(loc=3, scale=1, size=500)\n    labels = [0] * 500 + [1] * 500\n    data = np.concatenate([class1, class2])\n    return data, labels\n\n# Function to plot histograms\ndef plot_histograms(data_before, data_after, title_before, title_after):\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.hist(data_before, bins=30, edgecolor='black', alpha=0.7)\n    plt.title(title_before)\n    plt.xlabel('Values')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    plt.subplot(1, 2, 2)\n    plt.hist(data_after, bins=30, edgecolor='black', alpha=0.7)\n    plt.title(title_after)\n    plt.xlabel('Values')\n    plt.ylabel('Frequency')\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n\n# Step 1: Generate a random dataset\ndata_before, labels = generate_dataset()\n\n# Step 2: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data_before.reshape(-1, 1), labels, test_size=0.2, random_state=42)\n\n# Step 3: Train a Random Forest Classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Step 4: Make predictions on the test set\npredictions = clf.predict(X_test)\n\n# Step 5: Assess accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy of the model: {accuracy:.2%}\")\n\n# Step 6: Generate a dataset after applying the machine learning model\ndata_after = np.concatenate([clf.predict_proba(X_test)[:, 0], clf.predict_proba(X_test)[:, 1]])\n\n# Step 7: Plot histograms to compare before and after\nplot_histograms(data_before, data_after, 'Original Dataset', 'Dataset after Machine Learning')\n\nAccuracy of the model: 89.00%\n\n\n\n\n\nThis code generates a dataset with two classes, trains a Random Forest Classifier on the data, and then generates predictions on a test set. The histograms before and after applying the machine learning model are plotted side by side for comparison.\nThe generate_dataset function creates a synthetic dataset with two classes, and the plot_histograms function is used to visualize the distribution before and after applying the machine learning model. The accuracy of the model is also printed to give an idea of its performance."
  },
  {
    "objectID": "posts/new_blog_post/new_post.html",
    "href": "posts/new_blog_post/new_post.html",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]\n\n\n\n\n\n\n\n\ngeom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)\n\n\n\n\n\n\n\nggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/new_blog_post/new_post.html#merriweather",
    "href": "posts/new_blog_post/new_post.html#merriweather",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus.\n\n\nThis is inline code plus a small code chunk.\n\nlibrary(tidyverse)\n\nggplot(mpg) +\n  geom_jitter(aes(cty, hwy), size = 4, alpha = 0.5) \n\n\n\n\n\n\n\n\nTransforming OLS estimatesMaximizing likelihood\n\n\n\n\nCode\npreds_lm %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\nCode\nglm.mod &lt;- glm(sex ~ body_mass_g + bill_length_mm + species, family = binomial, data = dat)\n\npreds &lt;- dat %&gt;% \n  mutate(\n    prob.fit = glm.mod$fitted.values,\n    prediction = if_else(prob.fit &gt; 0.5, 'male', 'female'),\n    correct = if_else(sex == prediction, 'correct', 'incorrect')\n  )\n\n\npreds %&gt;% \n  ggplot(aes(body_mass_g, bill_length_mm, col = correct)) +\n  geom_jitter(size = 4, alpha = 0.6) +\n  facet_wrap(vars(species)) +\n  scale_x_continuous(breaks = seq(3000, 6000, 1000)) +\n  scale_color_manual(values = c('grey60', thematic::okabe_ito(3)[3])) +\n  theme_minimal(base_size = 10) +\n  theme(\n    legend.position = 'top', \n    panel.background = element_rect(color = 'black'),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(\n    x = 'Body mass (in g)',\n    y = 'Bill length (in mm)'\n  )\n\n\n\n\n\n\n\n\n\n\n\\[\n\\int_0^1 f(x) \\ dx\n\\]"
  },
  {
    "objectID": "posts/new_blog_post/new_post.html#columns",
    "href": "posts/new_blog_post/new_post.html#columns",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "geom_density(\n  mapping = NULL,\n  data = NULL,\n  stat = \"density\",\n  position = \"identity\",\n  ...,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE,\n  outline.type = \"upper\"\n)\n\n\nstat_density(\n  mapping = NULL,\n  data = NULL,\n  geom = \"area\",\n  position = \"stack\",\n  ...,\n  bw = \"nrd0\",\n  adjust = 1,\n  kernel = \"gaussian\",\n  n = 512,\n  trim = FALSE,\n  na.rm = FALSE,\n  orientation = NA,\n  show.legend = NA,\n  inherit.aes = TRUE\n)"
  },
  {
    "objectID": "posts/new_blog_post/new_post.html#margin-captions",
    "href": "posts/new_blog_post/new_post.html#margin-captions",
    "title": "This is a dummy blog posts",
    "section": "",
    "text": "ggplot(data = gapminder::gapminder, mapping = aes(x = lifeExp, fill = continent)) +\n  stat_density(position = \"identity\", alpha = 0.5)\n\n\n\n\nBla bla bla. This is a caption in the margin. Super cool isn’t it?"
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering, a fundamental technique in machine learning, is the process of grouping similar data points together. Its applications are diverse, ranging from customer segmentation in marketing to anomaly detection in cybersecurity. By identifying patterns within datasets, clustering enables us to gain valuable insights and make informed decisions."
  },
  {
    "objectID": "posts/clustering/index.html#introduction",
    "href": "posts/clustering/index.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "Clustering, a fundamental technique in machine learning, is the process of grouping similar data points together. Its applications are diverse, ranging from customer segmentation in marketing to anomaly detection in cybersecurity. By identifying patterns within datasets, clustering enables us to gain valuable insights and make informed decisions."
  },
  {
    "objectID": "posts/clustering/index.html#types-of-clustering-algorithms",
    "href": "posts/clustering/index.html#types-of-clustering-algorithms",
    "title": "Clustering",
    "section": "Types of Clustering Algorithms",
    "text": "Types of Clustering Algorithms\nClustering algorithms come in various flavors, each with its unique approach to grouping data:\n\nK-Means: Divides data into k clusters, where each cluster is represented by its centroid.\nHierarchical Clustering: Forms a tree of clusters, allowing for hierarchical organization.\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): A density-based algorithm that identifies clusters based on the density of data points."
  },
  {
    "objectID": "posts/clustering/index.html#dbscan-a-density-based-approach",
    "href": "posts/clustering/index.html#dbscan-a-density-based-approach",
    "title": "Clustering",
    "section": "DBSCAN: A Density-Based Approach:",
    "text": "DBSCAN: A Density-Based Approach:\nDBSCAN stands out for its ability to discover clusters of arbitrary shapes. The algorithm works by identifying core points, which are densely packed, and expanding clusters by connecting core points. It also identifies noise points that do not belong to any cluster."
  },
  {
    "objectID": "posts/clustering/index.html#dbscan-in-action",
    "href": "posts/clustering/index.html#dbscan-in-action",
    "title": "Clustering",
    "section": "DBSCAN in Action",
    "text": "DBSCAN in Action\nLet’s demonstrate the power of DBSCAN using Python and scikit-learn. We’ll start by generating a synthetic dataset:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# Generate a synthetic dataset\nX, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nlabels = dbscan.fit_predict(X)"
  },
  {
    "objectID": "posts/clustering/index.html#data-visualization-with-scatter-plots",
    "href": "posts/clustering/index.html#data-visualization-with-scatter-plots",
    "title": "Clustering",
    "section": "Data Visualization with Scatter Plots",
    "text": "Data Visualization with Scatter Plots\nVisualizing the results of clustering is crucial for understanding the underlying structure of the data. Let’s create a scatter plot that represents the original data points with cluster labels:\n\n# Visualize the clusters\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k', s=50)\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n\n\nClustering, especially with the powerful DBSCAN algorithm, opens new avenues for extracting patterns from data. As we’ve seen, DBSCAN’s ability to identify clusters based on density makes it versatile for a wide range of applications. By visualizing the results, we gain a deeper understanding of the data’s inherent structure, providing a solid foundation for subsequent analysis and decision-making."
  },
  {
    "objectID": "posts/anomaly/index.html",
    "href": "posts/anomaly/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection, a critical aspect of machine learning, involves identifying patterns in data that deviate significantly from the norm. This technique finds applications in various domains, from fraud detection in financial transactions to detecting anomalies in network traffic for cybersecurity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hannah Lyons Personal Blog"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Series",
    "section": "",
    "text": "Series\nmachine-learning-series\nThis series contains a great deal of code and data visualizations for anyone new to machine learning.\n\n\nAll Blog Posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\n\ncode\n\n\n\n\nUnveiling Anomalies with Outlier Detection in Machine Learning\n\n\n\n\n\n\nNov 5, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\ncode\n\n\n\n\nROC, PR, Confusion Matrix\n\n\n\n\n\n\nOct 20, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\ncode\n\n\n\n\nNavigating Trends with Linear and Nonlinear Regression in Machine Learning\n\n\n\n\n\n\nOct 5, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\ncode\n\n\n\n\nUnraveling Patterns with Clustering in Machine Learning\n\n\n\n\n\n\nSep 20, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\ncode\n\n\n\n\nUnderstanding Probability Theory, Random Variables, and Data Visualization in the Context of Machine Learning\n\n\n\n\n\n\nSep 5, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nTetris and Machine Learning\n\n\n\n\n\n\n\ncode\n\n\n\n\nMachine Learning and its applications to the classic videogame: Tetris\n\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nThis is a dummy blog posts\n\n\n\n\n\n\n\nnews\n\n\n\n\nThis is a test post. In this post, I try out different functionalities\n\n\n\n\n\n\nJun 1, 2022\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\n\n\n\n\n\n\nTetris and Machine Learning\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\n\n\n\n\n\n\nThis is a dummy blog posts\n\n\n\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ggplot-series.html",
    "href": "ggplot-series.html",
    "title": "Series: ggplot2-tips",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "machine-learning-series.html",
    "href": "machine-learning-series.html",
    "title": "Series: Introductory Machine Learning",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification, a cornerstone of machine learning, empowers systems to make informed decisions based on input features. From determining whether an email is spam to diagnosing diseases, classification algorithms play a pivotal role in automating decision-making processes."
  },
  {
    "objectID": "posts/intro-to-ml/index.html",
    "href": "posts/intro-to-ml/index.html",
    "title": "Tetris and Machine Learning",
    "section": "",
    "text": "Little did I know, Tetris, the classic video game, can be related to machine learning and data analytics in various ways.\nI am a huge fan of the game, and I am obsessed with a newer version that facilitates multi-player options; Tetris® Effect: Connected.\n\n\n\n\n\nBelow is a video demonstrating one of the newer multiplayer options, Zone Battle. This is me playing in the video (tacobaco32) and I like to say I’m pretty good at the game.\nVIDEO DEMONSTRATING TETRIS EFFECT\nI decided to stream on twitch a few times… that didn’t go anywhere. There are several other players that I’ve seen stream online, like doremypuyotet, who is a Tetris expert; the video below is them playing.\n[]()\nAs you can see doremypuyotet has incredible technique and it is absolutely mind blowing watching them play the game.\nI wondered though, how would an Artificial Tetris Agent perform at a game of zone battle against someone like doremypuyotet? The agent would have to take several more aspects into consideration than it does in the classic video game. The dynamics of player interactions, adaptive game play, and the wealth of data generated present opportunities to enhance the gaming experience through data-driven insights and algorithmic optimization’s.\n\n\n\n\nAlgorithmic Efficiency: Tetris involves optimizing the placement of falling blocks to complete lines. This is analogous to optimization problems in machine learning where algorithms seek to minimize or maximize a certain objective function.\nHeuristics: Players often develop heuristics or rules of thumb to make decisions quickly. Similarly, in machine learning, heuristics can be used to guide algorithms toward solutions without exhaustive search.\n\n\n\nLearning from Experience: Reinforcement learning involves agents learning by interacting with an environment and receiving feedback in the form of rewards or penalties. In Tetris, players learn from the consequences of their moves, much like reinforcement learning algorithms learn from trial and error.\n\n\n\nFeature Extraction: In Tetris, players recognize patterns in the falling blocks to make decisions. In data analytics, feature extraction involves recognizing patterns in data to enhance the performance of algorithms.\nImage Recognition: Tetris can be framed as an image recognition problem where the goal is to identify patterns in the arrangement of blocks. Image recognition is a common application in machine learning.\n\n\n\nAnticipating Future States: In Tetris, players need to anticipate the future states of the game based on the current configuration of blocks. Predictive analytics in machine learning involves forecasting future trends or outcomes based on historical data.\n\n\n\nReal-time Decision Making: Tetris requires quick decision-making based on the constantly changing game state. In a similar vein, real-time data analytics involves making decisions based on rapidly changing data streams.\n\n\n\nSpatial Visualization: Tetris involves visualizing the spatial arrangement of blocks. Data visualization is a crucial aspect of data analytics, helping individuals understand complex patterns and trends in data through graphical representations.\n\n\n\nGenetic Algorithms: Tetris can be approached as a problem of evolving a strategy over time. Genetic algorithms, a type of evolutionary algorithm, involve evolving solutions to problems through processes inspired by natural selection.\n\n\n\nComplexity Analysis: Analyzing the complexity of Tetris strategies can be similar to analyzing the time and space complexity of algorithms in machine learning. Understanding the efficiency and scalability of algorithms is essential in both contexts.\n\n\n\nManaging Information Overload: Tetris speeds up as the game progresses, leading to an increasing amount of information to process. Dealing with information overload is a challenge in both Tetris and big data analytics.\n\n\n\nGame Theory: Tetris can be viewed through the lens of game theory, where players make strategic decisions to maximize their score. Game theory concepts are also applicable in various areas of machine learning, such as in competitive scenarios."
  },
  {
    "objectID": "posts/intro-to-ml/index.html#tetris-applications-to-machine-learning",
    "href": "posts/intro-to-ml/index.html#tetris-applications-to-machine-learning",
    "title": "Tetris and Machine Learning",
    "section": "",
    "text": "Algorithmic Efficiency: Tetris involves optimizing the placement of falling blocks to complete lines. This is analogous to optimization problems in machine learning where algorithms seek to minimize or maximize a certain objective function.\nHeuristics: Players often develop heuristics or rules of thumb to make decisions quickly. Similarly, in machine learning, heuristics can be used to guide algorithms toward solutions without exhaustive search.\n\n\n\nLearning from Experience: Reinforcement learning involves agents learning by interacting with an environment and receiving feedback in the form of rewards or penalties. In Tetris, players learn from the consequences of their moves, much like reinforcement learning algorithms learn from trial and error.\n\n\n\nFeature Extraction: In Tetris, players recognize patterns in the falling blocks to make decisions. In data analytics, feature extraction involves recognizing patterns in data to enhance the performance of algorithms.\nImage Recognition: Tetris can be framed as an image recognition problem where the goal is to identify patterns in the arrangement of blocks. Image recognition is a common application in machine learning.\n\n\n\nAnticipating Future States: In Tetris, players need to anticipate the future states of the game based on the current configuration of blocks. Predictive analytics in machine learning involves forecasting future trends or outcomes based on historical data.\n\n\n\nReal-time Decision Making: Tetris requires quick decision-making based on the constantly changing game state. In a similar vein, real-time data analytics involves making decisions based on rapidly changing data streams.\n\n\n\nSpatial Visualization: Tetris involves visualizing the spatial arrangement of blocks. Data visualization is a crucial aspect of data analytics, helping individuals understand complex patterns and trends in data through graphical representations.\n\n\n\nGenetic Algorithms: Tetris can be approached as a problem of evolving a strategy over time. Genetic algorithms, a type of evolutionary algorithm, involve evolving solutions to problems through processes inspired by natural selection.\n\n\n\nComplexity Analysis: Analyzing the complexity of Tetris strategies can be similar to analyzing the time and space complexity of algorithms in machine learning. Understanding the efficiency and scalability of algorithms is essential in both contexts.\n\n\n\nManaging Information Overload: Tetris speeds up as the game progresses, leading to an increasing amount of information to process. Dealing with information overload is a challenge in both Tetris and big data analytics.\n\n\n\nGame Theory: Tetris can be viewed through the lens of game theory, where players make strategic decisions to maximize their score. Game theory concepts are also applicable in various areas of machine learning, such as in competitive scenarios."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression, a fundamental concept in machine learning, enables us to model relationships between variables. Whether predicting house prices based on square footage or analyzing the impact of advertising spend on sales, regression plays a pivotal role in understanding and predicting patterns in data."
  },
  {
    "objectID": "posts/regression/index.html#introduction",
    "href": "posts/regression/index.html#introduction",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression, a fundamental concept in machine learning, enables us to model relationships between variables. Whether predicting house prices based on square footage or analyzing the impact of advertising spend on sales, regression plays a pivotal role in understanding and predicting patterns in data."
  },
  {
    "objectID": "posts/regression/index.html#linear-regression",
    "href": "posts/regression/index.html#linear-regression",
    "title": "Linear and Nonlinear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is a straightforward method for modeling linear relationships between variables. In a simple linear regression model, the relationship is expressed as \\(y=mx+b\\) where \\(y\\) is the target variable, \\(x\\) is the feature, \\(m\\) is the slope, and \\(b\\) is the intercept. For multiple features, the equation becomes a linear combination.\nLinear regression can be implemented effortlessly using Python’s scikit-learn library. Let’s generate a synthetic dataset and visualize the linear regression line:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate synthetic linear data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Train linear regression model\nlinear_reg = LinearRegression()\nlinear_reg.fit(X, y)\n\n# Visualize the linear regression line\nplt.scatter(X, y, color='blue', alpha=0.6)\nplt.plot(X, linear_reg.predict(X), color='red', linewidth=2)\nplt.title('Linear Regression')\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.show()"
  },
  {
    "objectID": "posts/regression/index.html#nonlinear-regression",
    "href": "posts/regression/index.html#nonlinear-regression",
    "title": "Linear and Nonlinear Regression",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\nWhile linear regression is powerful, not all relationships are linear. Nonlinear regression extends the concept by accommodating more complex patterns. Polynomial regression is a common technique, where the relationship between variables is expressed as a polynomial equation.\nLet’s use Python to implement nonlinear regression through polynomial regression. This time, we’ll generate a synthetic dataset with a nonlinear relationship:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n# Generate synthetic nonlinear data\nnp.random.seed(42)\nX = 6 * np.random.rand(100, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\n# Train nonlinear regression model (polynomial regression)\ndegree = 2\npoly_reg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\npoly_reg.fit(X, y)\n\n# Visualize the nonlinear regression curve\nX_range = np.linspace(-3, 3, 100).reshape(-1, 1)\nplt.scatter(X, y, color='blue', alpha=0.6)\nplt.plot(X_range, poly_reg.predict(X_range), color='red', linewidth=2)\nplt.title('Nonlinear Regression (Polynomial)')\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.show()\n\n\n\n\nThe power of regression becomes evident when we visualize the results. Scatter plots with regression lines or curves help us understand how well the model captures the underlying patterns in the data.\nIn conclusion, linear and nonlinear regression are indispensable tools in the machine learning toolbox. While linear regression is effective for simple relationships, nonlinear regression allows us to model more complex patterns. The ability to implement these techniques with Python makes them accessible and applicable to a wide range of real-world scenarios."
  },
  {
    "objectID": "posts/classification/index.html#introduction",
    "href": "posts/classification/index.html#introduction",
    "title": "Classification",
    "section": "",
    "text": "Classification, a cornerstone of machine learning, empowers systems to make informed decisions based on input features. From determining whether an email is spam to diagnosing diseases, classification algorithms play a pivotal role in automating decision-making processes."
  },
  {
    "objectID": "posts/classification/index.html#types-of-classification-algorithms",
    "href": "posts/classification/index.html#types-of-classification-algorithms",
    "title": "Classification",
    "section": "Types of Classification Algorithms",
    "text": "Types of Classification Algorithms\nThere are several classification algorithms, each suited to different types of problems:\n\nLogistic Regression: Ideal for binary classification tasks.\nDecision Trees: Effective for both binary and multiclass classification.\nSupport Vector Machines (SVM): Robust for linear and nonlinear classification.\n\nLet’s implement a simple classification model using logistic regression in Python:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n\n# Generate synthetic classification data\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = (X &gt; 0.5).astype(int).flatten()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_proba = logreg.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "posts/classification/index.html#receiver-operating-characteristic-roc-curve",
    "href": "posts/classification/index.html#receiver-operating-characteristic-roc-curve",
    "title": "Classification",
    "section": "Receiver Operating Characteristic (ROC) Curve",
    "text": "Receiver Operating Characteristic (ROC) Curve\nROC curves visualize the trade-off between true positive rate (sensitivity) and false positive rate. The area under the ROC curve (AUC-ROC) is a valuable metric for model performance.\n\n# Visualize the ROC curve\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()"
  },
  {
    "objectID": "posts/classification/index.html#precision-recall-pr-curve",
    "href": "posts/classification/index.html#precision-recall-pr-curve",
    "title": "Classification",
    "section": "Precision-Recall (PR) Curve",
    "text": "Precision-Recall (PR) Curve\nPR curves focus on the trade-off between precision and recall, particularly valuable in imbalanced datasets.\n\n# Visualize the Precision-Recall curve\nprecision, recall, _ = precision_recall_curve(y_test, y_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, color='green', lw=2, label='Precision-Recall curve')\nplt.xlabel('Recall (Sensitivity)')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc='lower left')\nplt.show()"
  },
  {
    "objectID": "posts/classification/index.html#confusion-matrix",
    "href": "posts/classification/index.html#confusion-matrix",
    "title": "Classification",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nThe confusion matrix provides a detailed understanding of a classification model’s performance, breaking down predictions into true positives, true negatives, false positives, and false negatives.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Assuming logreg, X_test, and y_test are defined as in the previous code\n\n# Generate predictions\ny_pred = logreg.predict(X_test)\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualize the confusion matrix using matplotlib\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = ['Class 0', 'Class 1']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\n\nplt.show()\n\n\n\n\nIn conclusion, classification in machine learning is a powerful tool for automating decision-making processes. By implementing and evaluating classification models, we gain insights into their performance through metrics like ROC curves, PR curves, and confusion matrices. These visualizations provide a nuanced understanding of a model’s strengths and weaknesses, facilitating informed decision-making in real-world applications."
  },
  {
    "objectID": "posts/anomaly/index.html#introduction",
    "href": "posts/anomaly/index.html#introduction",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection, a critical aspect of machine learning, involves identifying patterns in data that deviate significantly from the norm. This technique finds applications in various domains, from fraud detection in financial transactions to detecting anomalies in network traffic for cybersecurity."
  },
  {
    "objectID": "posts/anomaly/index.html#challenges-of-anomaly-detection",
    "href": "posts/anomaly/index.html#challenges-of-anomaly-detection",
    "title": "Anomaly/Outlier Detection",
    "section": "Challenges of Anomaly Detection",
    "text": "Challenges of Anomaly Detection\nWhile anomaly detection is a powerful tool, it comes with challenges. Real-world data often exhibits diverse distributions, and datasets can be imbalanced, making it tricky to identify rare anomalies accurately. Robust algorithms are essential to overcome these challenges."
  },
  {
    "objectID": "posts/anomaly/index.html#dbscan-for-anomaly-detection",
    "href": "posts/anomaly/index.html#dbscan-for-anomaly-detection",
    "title": "Anomaly/Outlier Detection",
    "section": "DBSCAN for Anomaly Detection",
    "text": "DBSCAN for Anomaly Detection\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) emerges as a robust algorithm for anomaly detection. Unlike traditional methods, DBSCAN doesn’t assume a specific shape for clusters and can effectively isolate points in low-density regions as outliers.\nLet’s implement anomaly detection using DBSCAN in Python:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# Function to generate a synthetic dataset with outliers\ndef generate_dataset_with_outliers(seed=42):\n    np.random.seed(seed)\n    X, _ = make_blobs(n_samples=300, centers=1, cluster_std=1, random_state=0)\n    outliers = np.random.uniform(low=-10, high=10, size=(20, 2))\n    return np.concatenate([X, outliers]), np.concatenate([np.zeros(300), np.ones(20)])\n\n# Function to visualize anomalies with DBSCAN labels\ndef plot_anomalies(X, labels, title):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k', s=50)\n    plt.title(title)\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.show()\n\n# Step 1: Generate a synthetic dataset with outliers\nX, y = generate_dataset_with_outliers()\n\n# Step 2: Train a DBSCAN model for anomaly detection\ndbscan = DBSCAN(eps=1, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\n# Step 3: Visualize anomalies with DBSCAN labels\nplot_anomalies(X, labels, 'Anomaly Detection with DBSCAN')\n\n\n\n\nVisualizing anomalies is crucial for interpreting model results. Scatter plots with DBSCAN labels provide an intuitive representation of the identified anomalies in the dataset."
  },
  {
    "objectID": "posts/anomaly/index.html#evaluation-metrics-for-anomaly-detection",
    "href": "posts/anomaly/index.html#evaluation-metrics-for-anomaly-detection",
    "title": "Anomaly/Outlier Detection",
    "section": "Evaluation Metrics for Anomaly Detection",
    "text": "Evaluation Metrics for Anomaly Detection\nEvaluating anomaly detection models poses challenges due to the scarcity of anomalies. Metrics such as precision, recall, and F1 score provide a nuanced understanding of the model’s performance.\nLet’s calculate precision, recall, and F1 score for anomaly detection results:\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to generate a synthetic dataset with outliers\ndef generate_dataset_with_outliers(seed=42):\n    np.random.seed(seed)\n    X, _ = make_blobs(n_samples=300, centers=1, cluster_std=1, random_state=0)\n    outliers = np.random.uniform(low=-10, high=10, size=(20, 2))\n    return np.concatenate([X, outliers]), np.concatenate([np.zeros(300), -np.ones(20)])\n\n# Function to visualize anomalies with DBSCAN labels\ndef plot_anomalies(X, labels, title):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolor='k', s=50)\n    plt.title(title)\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.show()\n\n# Step 1: Generate a synthetic dataset with outliers\nX, y_true = generate_dataset_with_outliers()\n\n# Step 2: Train a DBSCAN model for anomaly detection\ndbscan = DBSCAN(eps=1, min_samples=5)\npredicted_labels = dbscan.fit_predict(X)\n\n# Step 3: Visualize anomalies with DBSCAN labels\nplot_anomalies(X, predicted_labels, 'Anomaly Detection with DBSCAN')\n\n# Step 4: Evaluate the model using precision, recall, and F1 score\nprecision = precision_score(y_true, predicted_labels, pos_label=-1)\nrecall = recall_score(y_true, predicted_labels, pos_label=-1)\nf1 = f1_score(y_true, predicted_labels, pos_label=-1)\n\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\n\n\n\nPrecision: 1.00\nRecall: 0.90\nF1 Score: 0.95\n\n\nAnomaly detection, powered by algorithms like DBSCAN, plays a vital role in uncovering irregularities in data. The combination of effective visualizations and evaluation metrics enables us to gain insights into the anomalies present, providing a foundation for critical decision-making in various domains."
  }
]