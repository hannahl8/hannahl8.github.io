[
  {
    "objectID": "posts/machine-learning-intro/regression/index.html",
    "href": "posts/machine-learning-intro/regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression, a fundamental concept in machine learning, enables us to model relationships between variables. Whether predicting house prices based on square footage or analyzing the impact of advertising spend on sales, regression plays a pivotal role in understanding and predicting patterns in data."
  },
  {
    "objectID": "posts/machine-learning-intro/regression/index.html#introduction",
    "href": "posts/machine-learning-intro/regression/index.html#introduction",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression, a fundamental concept in machine learning, enables us to model relationships between variables. Whether predicting house prices based on square footage or analyzing the impact of advertising spend on sales, regression plays a pivotal role in understanding and predicting patterns in data."
  },
  {
    "objectID": "posts/machine-learning-intro/regression/index.html#data-visualization",
    "href": "posts/machine-learning-intro/regression/index.html#data-visualization",
    "title": "Linear and Nonlinear Regression",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\nI will use the following plot_scatter function to create the data visualizations on this page\n\nimport matplotlib.pyplot as plt\n\ndef plot_scatter(X, X_Range, ax_plot, title):\n  fig, ax = plt.subplots()\n  scatter = ax.scatter(X, y, color=blue, edgecolor='black', alpha=0.6)\n  ax.plot(X_Range, ax_plot, color=purple, linewidth=2)\n  ax.set_title(title, color=purple)\n  ax.set_xlabel('Feature', color=blue)\n  ax.set_ylabel('Target', color=blue)\n  ax.grid(True, linestyle='--', alpha=0.5)\n  ax.tick_params(axis='x', colors=blue)\n  ax.tick_params(axis='y', colors=blue)\n  ax.set_facecolor(pink)\n  fig.set_facecolor(background)\n  plt.show()\n\n\nLinear Regression\nLinear regression is a straightforward method for modeling linear relationships between variables. In a simple linear regression model, the relationship is expressed as \\(y=mx+b\\) where \\(y\\) is the target variable, \\(x\\) is the feature, \\(m\\) is the slope, and \\(b\\) is the intercept. For multiple features, the equation becomes a linear combination.\nLinear regression can be implemented effortlessly using Python’s scikit-learn library. Let’s generate a synthetic dataset and visualize the linear regression line:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Generate synthetic linear data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Train linear regression model\nlinear_reg = LinearRegression()\nlinear_reg.fit(X, y)\n\n# Visualize the linear regression line\nax_plot = linear_reg.predict(X)\nplot_scatter(X, X, ax_plot, 'Linear Regression')\n\n\n\n\n\n\nNonlinear Regression\nWhile linear regression is powerful, not all relationships are linear. Nonlinear regression extends the concept by accommodating more complex patterns. Polynomial regression is a common technique, where the relationship between variables is expressed as a polynomial equation.\nLet’s use Python to implement nonlinear regression through polynomial regression. This time, we’ll generate a synthetic dataset with a nonlinear relationship:\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n# Generate synthetic nonlinear data\nnp.random.seed(42)\nX = 6 * np.random.rand(100, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\n# Train nonlinear regression model (polynomial regression)\ndegree = 2\npoly_reg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\npoly_reg.fit(X, y)\n\n# Visualize the nonlinear regression curve\nX_range = np.linspace(-3, 3, 100).reshape(-1, 1)\nax_plot =  poly_reg.predict(X_range)\nplot_scatter(X, X_range, ax_plot, 'Nonlinear Regression (Polynomial)')\n\n\n\n\nThe power of regression becomes evident when we visualize the results. Scatter plots with regression lines or curves help us understand how well the model captures the underlying patterns in the data.\nIn conclusion, linear and nonlinear regression are indispensable tools in the machine learning toolbox. While linear regression is effective for simple relationships, nonlinear regression allows us to model more complex patterns. The ability to implement these techniques with Python makes them accessible and applicable to a wide range of real-world scenarios."
  },
  {
    "objectID": "posts/machine-learning-intro/intro-to-ml/index.html",
    "href": "posts/machine-learning-intro/intro-to-ml/index.html",
    "title": "Tetris and Machine Learning",
    "section": "",
    "text": "Little did I know, Tetris, the classic video game, can be related to machine learning and data analytics in various ways.\nI am a huge fan of the game, and I am obsessed with a newer version that facilitates multi-player options; Tetris® Effect: Connected.\n\n\n\n\n\nI decided to stream on twitch a few times… that didn’t go anywhere. There are several other players that I’ve seen stream online, like doremypuyotet, who is a Tetris expert; the video below is them playing.\n[]()\nAs you can see doremypuyotet has incredible technique and it is absolutely mind blowing watching them play the game.\nI wondered though, how would an Artificial Tetris Agent perform at a game of zone battle against someone like doremypuyotet? The agent would have to take several more aspects into consideration than it does in the classic video game. The dynamics of player interactions, adaptive game play, and the wealth of data generated present opportunities to enhance the gaming experience through data-driven insights and algorithmic optimization’s.\n\n\n\n\nAlgorithmic Efficiency: Tetris involves optimizing the placement of falling blocks to complete lines. This is analogous to optimization problems in machine learning where algorithms seek to minimize or maximize a certain objective function.\nHeuristics: Players often develop heuristics or rules of thumb to make decisions quickly. Similarly, in machine learning, heuristics can be used to guide algorithms toward solutions without exhaustive search.\n\n\n\nLearning from Experience: Reinforcement learning involves agents learning by interacting with an environment and receiving feedback in the form of rewards or penalties. In Tetris, players learn from the consequences of their moves, much like reinforcement learning algorithms learn from trial and error.\n\n\n\nFeature Extraction: In Tetris, players recognize patterns in the falling blocks to make decisions. In data analytics, feature extraction involves recognizing patterns in data to enhance the performance of algorithms.\nImage Recognition: Tetris can be framed as an image recognition problem where the goal is to identify patterns in the arrangement of blocks. Image recognition is a common application in machine learning.\n\n\n\nAnticipating Future States: In Tetris, players need to anticipate the future states of the game based on the current configuration of blocks. Predictive analytics in machine learning involves forecasting future trends or outcomes based on historical data.\n\n\n\nReal-time Decision Making: Tetris requires quick decision-making based on the constantly changing game state. In a similar vein, real-time data analytics involves making decisions based on rapidly changing data streams.\n\n\n\nSpatial Visualization: Tetris involves visualizing the spatial arrangement of blocks. Data visualization is a crucial aspect of data analytics, helping individuals understand complex patterns and trends in data through graphical representations.\n\n\n\nGenetic Algorithms: Tetris can be approached as a problem of evolving a strategy over time. Genetic algorithms, a type of evolutionary algorithm, involve evolving solutions to problems through processes inspired by natural selection.\n\n\n\nComplexity Analysis: Analyzing the complexity of Tetris strategies can be similar to analyzing the time and space complexity of algorithms in machine learning. Understanding the efficiency and scalability of algorithms is essential in both contexts.\n\n\n\nManaging Information Overload: Tetris speeds up as the game progresses, leading to an increasing amount of information to process. Dealing with information overload is a challenge in both Tetris and big data analytics.\n\n\n\nGame Theory: Tetris can be viewed through the lens of game theory, where players make strategic decisions to maximize their score. Game theory concepts are also applicable in various areas of machine learning, such as in competitive scenarios."
  },
  {
    "objectID": "posts/machine-learning-intro/intro-to-ml/index.html#tetris-applications-to-machine-learning",
    "href": "posts/machine-learning-intro/intro-to-ml/index.html#tetris-applications-to-machine-learning",
    "title": "Tetris and Machine Learning",
    "section": "",
    "text": "Algorithmic Efficiency: Tetris involves optimizing the placement of falling blocks to complete lines. This is analogous to optimization problems in machine learning where algorithms seek to minimize or maximize a certain objective function.\nHeuristics: Players often develop heuristics or rules of thumb to make decisions quickly. Similarly, in machine learning, heuristics can be used to guide algorithms toward solutions without exhaustive search.\n\n\n\nLearning from Experience: Reinforcement learning involves agents learning by interacting with an environment and receiving feedback in the form of rewards or penalties. In Tetris, players learn from the consequences of their moves, much like reinforcement learning algorithms learn from trial and error.\n\n\n\nFeature Extraction: In Tetris, players recognize patterns in the falling blocks to make decisions. In data analytics, feature extraction involves recognizing patterns in data to enhance the performance of algorithms.\nImage Recognition: Tetris can be framed as an image recognition problem where the goal is to identify patterns in the arrangement of blocks. Image recognition is a common application in machine learning.\n\n\n\nAnticipating Future States: In Tetris, players need to anticipate the future states of the game based on the current configuration of blocks. Predictive analytics in machine learning involves forecasting future trends or outcomes based on historical data.\n\n\n\nReal-time Decision Making: Tetris requires quick decision-making based on the constantly changing game state. In a similar vein, real-time data analytics involves making decisions based on rapidly changing data streams.\n\n\n\nSpatial Visualization: Tetris involves visualizing the spatial arrangement of blocks. Data visualization is a crucial aspect of data analytics, helping individuals understand complex patterns and trends in data through graphical representations.\n\n\n\nGenetic Algorithms: Tetris can be approached as a problem of evolving a strategy over time. Genetic algorithms, a type of evolutionary algorithm, involve evolving solutions to problems through processes inspired by natural selection.\n\n\n\nComplexity Analysis: Analyzing the complexity of Tetris strategies can be similar to analyzing the time and space complexity of algorithms in machine learning. Understanding the efficiency and scalability of algorithms is essential in both contexts.\n\n\n\nManaging Information Overload: Tetris speeds up as the game progresses, leading to an increasing amount of information to process. Dealing with information overload is a challenge in both Tetris and big data analytics.\n\n\n\nGame Theory: Tetris can be viewed through the lens of game theory, where players make strategic decisions to maximize their score. Game theory concepts are also applicable in various areas of machine learning, such as in competitive scenarios."
  },
  {
    "objectID": "posts/machine-learning-intro/classification/index.html",
    "href": "posts/machine-learning-intro/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification, a cornerstone of machine learning, empowers systems to make informed decisions based on input features. From determining whether an email is spam to diagnosing diseases, classification algorithms play a pivotal role in automating decision-making processes.\n\n\nThere are several classification algorithms, each suited to different types of problems:\n\nLogistic Regression: Ideal for binary classification tasks.\nDecision Trees: Effective for both binary and multiclass classification.\nSupport Vector Machines (SVM): Robust for linear and nonlinear classification.\n\nLet’s implement a simple classification model using logistic regression in Python:\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n\n# Generate synthetic classification data\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = (X &gt; 0.5).astype(int).flatten()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_proba = logreg.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "posts/machine-learning-intro/classification/index.html#introduction",
    "href": "posts/machine-learning-intro/classification/index.html#introduction",
    "title": "Classification",
    "section": "",
    "text": "Classification, a cornerstone of machine learning, empowers systems to make informed decisions based on input features. From determining whether an email is spam to diagnosing diseases, classification algorithms play a pivotal role in automating decision-making processes.\n\n\nThere are several classification algorithms, each suited to different types of problems:\n\nLogistic Regression: Ideal for binary classification tasks.\nDecision Trees: Effective for both binary and multiclass classification.\nSupport Vector Machines (SVM): Robust for linear and nonlinear classification.\n\nLet’s implement a simple classification model using logistic regression in Python:\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n\n# Generate synthetic classification data\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = (X &gt; 0.5).astype(int).flatten()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_proba = logreg.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "posts/machine-learning-intro/classification/index.html#data-visualization",
    "href": "posts/machine-learning-intro/classification/index.html#data-visualization",
    "title": "Classification",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\n\nReceiver Operating Characteristic (ROC) Curve\nROC curves visualize the trade-off between true positive rate (sensitivity) and false positive rate. The area under the ROC curve (AUC-ROC) is a valuable metric for model performance.\n\nimport matplotlib.pyplot as plt\n\n# Visualize the ROC curve\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nfig, ax = plt.subplots()\nax.plot(fpr, tpr, color=blue, lw=2, label='ROC curve')\nax.plot([0, 1], [0, 1], color=purple, lw=2, linestyle='--', label='Random Guess')\nax.set_xlabel('False Positive Rate', color=blue)\nax.set_ylabel('True Positive Rate', color=blue)\nax.set_title('Receiver Operating Characteristic (ROC) Curve', color=purple)\nax.legend(loc='lower right')\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\n\n\nPrecision-Recall (PR) Curve\nPR curves focus on the trade-off between precision and recall, particularly valuable in imbalanced datasets.\n\n# Visualize the Precision-Recall curve\nprecision, recall, _ = precision_recall_curve(y_test, y_proba)\nfig, ax = plt.subplots()\nax.plot(recall, precision, color=blue, lw=2, label='Precision-Recall curve')\nax.set_xlabel('Recall (Sensitivity)', color=blue)\nax.set_ylabel('Precision', color=blue)\nax.set_title('Precision-Recall Curve', color=purple)\nax.legend(loc='lower left')\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\n\n\nConfusion Matrix\nThe confusion matrix provides a detailed understanding of a classification model’s performance, breaking down predictions into true positives, true negatives, false positives, and false negatives.\n\nfrom sklearn.metrics import confusion_matrix\n\n# Generate predictions\ny_pred = logreg.predict(X_test)\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualize the confusion matrix\nfig, ax = plt.subplots()\ncax = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.cool)\nax.set_title('Confusion Matrix', color=purple)\nplt.colorbar(cax)\nclasses = ['Class 0', 'Class 1']\ntick_marks = np.arange(len(classes))\nax.set_xticks(tick_marks)\nax.set_yticks(tick_marks)\nax.set_xticklabels(classes, rotation=45, color=blue)\nax.set_yticklabels(classes, color=blue)\nax.set_xlabel('Predicted label', color=blue)\nax.set_ylabel('True label', color=blue)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\nIn conclusion, classification in machine learning is a powerful tool for automating decision-making processes. By implementing and evaluating classification models, we gain insights into their performance through metrics like ROC curves, PR curves, and confusion matrices. These visualizations provide a nuanced understanding of a model’s strengths and weaknesses, facilitating informed decision-making in real-world applications."
  },
  {
    "objectID": "machine-learning-series.html",
    "href": "machine-learning-series.html",
    "title": "Series: Introductory Machine Learning",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\nUnveiling Anomalies with Outlier Detection in Machine Learning\n\n\n\n\n\n\nNov 20, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\nROC, PR, Confusion Matrix\n\n\n\n\n\n\nOct 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\nNavigating Trends with Linear and Nonlinear Regression in Machine Learning\n\n\n\n\n\n\nOct 20, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\nUnraveling Patterns with Clustering in Machine Learning\n\n\n\n\n\n\nSep 30, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\nUnderstanding Probability Theory and Random Variables in the Context of Machine Learning\n\n\n\n\n\n\nSep 20, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nTetris and Machine Learning\n\n\n\n\n\nMachine Learning and its applications to the classic videogame: Tetris\n\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Series",
    "section": "",
    "text": "Series\nmachine-learning-series\nThis series contains a great deal of code and data visualizations for anyone new to machine learning.\n\n\nAll Blog Posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\n\ncode\n\n\n\n\nUnveiling Anomalies with Outlier Detection in Machine Learning\n\n\n\n\n\n\nNov 20, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\ncode\n\n\n\n\nROC, PR, Confusion Matrix\n\n\n\n\n\n\nOct 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\ncode\n\n\n\n\nNavigating Trends with Linear and Nonlinear Regression in Machine Learning\n\n\n\n\n\n\nOct 20, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\ncode\n\n\n\n\nUnraveling Patterns with Clustering in Machine Learning\n\n\n\n\n\n\nSep 30, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\ncode\n\n\n\n\nUnderstanding Probability Theory and Random Variables in the Context of Machine Learning\n\n\n\n\n\n\nSep 20, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nTetris and Machine Learning\n\n\n\n\n\n\n\ntetris\n\n\n\n\nMachine Learning and its applications to the classic videogame: Tetris\n\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\nSep 30, 2023\n\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\n\n\n\n\n\n\nTetris and Machine Learning\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hello all! My name is Hannah Lyons. I graduated from Virginia Tech with a Bachelor’s in Biochemistry, a minor in Computer Science, and a minor in Chemistry in May 2021. Since then I have been working full time as a Computer Scientist at Naval Surface Warefare Center Indian Head Division on the Modeling & Simulations Team. August 2023, I started working part time so I could pursue a Master of Engineering in Computer Science, also at Virignia Tech. Below are links to a few projects I have worked on during my career as a graduate student!\nI am super passionate about both front and back end web development (full stack), as well as general software development for desktop applications, etc.\n\nProject Links\nHokie Housing Hub\nfocal\nBlog"
  },
  {
    "objectID": "posts/machine-learning-intro/anomaly/index.html",
    "href": "posts/machine-learning-intro/anomaly/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection, a critical aspect of machine learning, involves identifying patterns in data that deviate significantly from the norm. This technique finds applications in various domains, from fraud detection in financial transactions to detecting anomalies in network traffic for cybersecurity.\n\n\nWhile anomaly detection is a powerful tool, it comes with challenges. Real-world data often exhibits diverse distributions, and datasets can be imbalanced, making it tricky to identify rare anomalies accurately. Robust algorithms are essential to overcome these challenges."
  },
  {
    "objectID": "posts/machine-learning-intro/anomaly/index.html#introduction",
    "href": "posts/machine-learning-intro/anomaly/index.html#introduction",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection, a critical aspect of machine learning, involves identifying patterns in data that deviate significantly from the norm. This technique finds applications in various domains, from fraud detection in financial transactions to detecting anomalies in network traffic for cybersecurity.\n\n\nWhile anomaly detection is a powerful tool, it comes with challenges. Real-world data often exhibits diverse distributions, and datasets can be imbalanced, making it tricky to identify rare anomalies accurately. Robust algorithms are essential to overcome these challenges."
  },
  {
    "objectID": "posts/machine-learning-intro/anomaly/index.html#data-visualization",
    "href": "posts/machine-learning-intro/anomaly/index.html#data-visualization",
    "title": "Anomaly/Outlier Detection",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\n\nDBSCAN and Evaluation Metrics for Anomaly Detection\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) emerges as a robust algorithm for anomaly detection. Unlike traditional methods, DBSCAN doesn’t assume a specific shape for clusters and can effectively isolate points in low-density regions as outliers.\nVisualizing anomalies is crucial for interpreting model results. Scatter plots with DBSCAN labels provide an intuitive representation of the identified anomalies in the dataset.\nEvaluating anomaly detection models poses challenges due to the scarcity of anomalies. Metrics such as precision, recall, and F1 score provide a nuanced understanding of the model’s performance.\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to generate a synthetic dataset with outliers\ndef generate_dataset_with_outliers(seed=42):\n    np.random.seed(seed)\n    X, _ = make_blobs(n_samples=300, centers=1, cluster_std=1, random_state=0)\n    outliers = np.random.uniform(low=-10, high=10, size=(20, 2))\n    return np.concatenate([X, outliers]), np.concatenate([np.zeros(300), -np.ones(20)])\n\n# Step 1: Generate a synthetic dataset with outliers\nX, y_true = generate_dataset_with_outliers()\n\n# Step 2: Train a DBSCAN model for anomaly detection\ndbscan = DBSCAN(eps=1, min_samples=5)\npredicted_labels = dbscan.fit_predict(X)\n\n# Step 3: Visualize anomalies with DBSCAN labels\nfig, ax = plt.subplots()\nscatter = ax.scatter(X[:, 0], X[:, 1], c=predicted_labels, cmap=plt.cm.cool, edgecolor='black', s=50)\nax.set_title('Anomaly Detection with DBSCAN', color=purple)\nax.set_xlabel('Feature 1', color=blue)\nax.set_ylabel('Feature 2', color=blue)\nax.grid(True, linestyle='--', alpha=0.5)\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n# Step 4: Evaluate the model using precision, recall, and F1 score\nprecision = precision_score(y_true, predicted_labels, pos_label=-1)\nrecall = recall_score(y_true, predicted_labels, pos_label=-1)\nf1 = f1_score(y_true, predicted_labels, pos_label=-1)\n\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\n\n\n\nPrecision: 1.00\nRecall: 0.90\nF1 Score: 0.95\n\n\nAnomaly detection, powered by algorithms like DBSCAN, plays a vital role in uncovering irregularities in data. The combination of effective visualizations and evaluation metrics enables us to gain insights into the anomalies present, providing a foundation for critical decision-making in various domains."
  },
  {
    "objectID": "posts/machine-learning-intro/clustering/index.html",
    "href": "posts/machine-learning-intro/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering, a fundamental technique in machine learning, is the process of grouping similar data points together. Its applications are diverse, ranging from customer segmentation in marketing to anomaly detection in cybersecurity. By identifying patterns within datasets, clustering enables us to gain valuable insights and make informed decisions.\n\n\nClustering algorithms come in various flavors, each with its unique approach to grouping data:\n\nK-Means: Divides data into k clusters, where each cluster is represented by its centroid.\nHierarchical Clustering: Forms a tree of clusters, allowing for hierarchical organization.\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): A density-based algorithm that identifies clusters based on the density of data points.\n\nDBSCAN stands out for its ability to discover clusters of arbitrary shapes. The algorithm works by identifying core points, which are densely packed, and expanding clusters by connecting core points. It also identifies noise points that do not belong to any cluster."
  },
  {
    "objectID": "posts/machine-learning-intro/clustering/index.html#introduction",
    "href": "posts/machine-learning-intro/clustering/index.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "Clustering, a fundamental technique in machine learning, is the process of grouping similar data points together. Its applications are diverse, ranging from customer segmentation in marketing to anomaly detection in cybersecurity. By identifying patterns within datasets, clustering enables us to gain valuable insights and make informed decisions.\n\n\nClustering algorithms come in various flavors, each with its unique approach to grouping data:\n\nK-Means: Divides data into k clusters, where each cluster is represented by its centroid.\nHierarchical Clustering: Forms a tree of clusters, allowing for hierarchical organization.\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): A density-based algorithm that identifies clusters based on the density of data points.\n\nDBSCAN stands out for its ability to discover clusters of arbitrary shapes. The algorithm works by identifying core points, which are densely packed, and expanding clusters by connecting core points. It also identifies noise points that do not belong to any cluster."
  },
  {
    "objectID": "posts/machine-learning-intro/clustering/index.html#data-visualization",
    "href": "posts/machine-learning-intro/clustering/index.html#data-visualization",
    "title": "Clustering",
    "section": "Data Visualization",
    "text": "Data Visualization\nLet’s demonstrate the power of DBSCAN using Python and scikit-learn. We’ll start by generating a synthetic dataset:\n\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# Generate a synthetic dataset\nX, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\nVisualizing the results of clustering is crucial for understanding the underlying structure of the data. Let’s create a scatter plot that represents the original data points with cluster labels:\n\nimport matplotlib.pyplot as plt\n\n# Visualize the clustering\nfig, ax = plt.subplots()\nscatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap=plt.cm.cool, edgecolor='black', s=50)\nax.set_title('DBSCAN Clustering', color=purple)\nax.set_xlabel('Feature 1', color=blue)\nax.set_ylabel('Feature 2', color=blue)\nax.grid(True, linestyle='--', alpha=0.5)\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\nClustering, especially with the powerful DBSCAN algorithm, opens new avenues for extracting patterns from data. As we’ve seen, DBSCAN’s ability to identify clusters based on density makes it versatile for a wide range of applications. By visualizing the results, we gain a deeper understanding of the data’s inherent structure, providing a solid foundation for subsequent analysis and decision-making."
  },
  {
    "objectID": "posts/machine-learning-intro/probability/index.html",
    "href": "posts/machine-learning-intro/probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In the realm of machine learning, uncertainty is inherent. Models make predictions based on patterns learned from data, and probability theory provides a formal framework for dealing with uncertainty. Random variables are employed to represent uncertain quantities, making probability theory a cornerstone for building and interpreting machine learning models.\nMachine learning algorithms often assume specific probability distributions. For instance:\n\nThe normal distribution is frequently assumed in linear regression and Gaussian processes.\nThe binomial distribution is relevant for problems involving binary outcomes, such as classification tasks.\n\nUnderstanding the characteristics of these distributions is vital when selecting appropriate models and interpreting results.\nRandom variables represent various aspects of data. Features can be modeled as random variables, and understanding their distribution is crucial for feature engineering. Additionally, the uncertainty associated with predictions is often expressed through probability distributions, making random variables central to the prediction process."
  },
  {
    "objectID": "posts/machine-learning-intro/probability/index.html#introduction",
    "href": "posts/machine-learning-intro/probability/index.html#introduction",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In the realm of machine learning, uncertainty is inherent. Models make predictions based on patterns learned from data, and probability theory provides a formal framework for dealing with uncertainty. Random variables are employed to represent uncertain quantities, making probability theory a cornerstone for building and interpreting machine learning models.\nMachine learning algorithms often assume specific probability distributions. For instance:\n\nThe normal distribution is frequently assumed in linear regression and Gaussian processes.\nThe binomial distribution is relevant for problems involving binary outcomes, such as classification tasks.\n\nUnderstanding the characteristics of these distributions is vital when selecting appropriate models and interpreting results.\nRandom variables represent various aspects of data. Features can be modeled as random variables, and understanding their distribution is crucial for feature engineering. Additionally, the uncertainty associated with predictions is often expressed through probability distributions, making random variables central to the prediction process."
  },
  {
    "objectID": "posts/machine-learning-intro/probability/index.html#data-visualization",
    "href": "posts/machine-learning-intro/probability/index.html#data-visualization",
    "title": "Probability Theory and Random Variables",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\nI will use the following plot_histograms function to create the data visualizations on this page\n\nimport matplotlib.pyplot as plt\n\n# Function to plot histograms\ndef plot_histograms(data, title):\n    # Create a histogram\n    fig, ax = plt.subplots()\n    hist = ax.hist(data, bins=30, edgecolor='black', alpha=0.7, color=purple, ec=blue)\n    ax.set_title(title, color=purple)\n    ax.set_xlabel('Values', color=blue)\n    ax.set_ylabel('Frequency', color=blue)\n    ax.grid(True, linestyle='--', alpha=0.5)\n    ax.tick_params(axis='x', colors=blue)\n    ax.tick_params(axis='y', colors=blue)\n    \n    ax.set_facecolor(pink)\n    fig.set_facecolor(background)\n    \n    # Show the plot\n    plt.show()\n\nLet’s use Python to create a histogram of a simulated random variable:\n\nimport numpy as np\n\n# Sets the seed for the random number generator to 42\nnp.random.seed(42)\n# Generates an array of 1000 random numbers drawn from a normal distribution\n# using the random number generated defined above\ndata = np.random.normal(size=1000)\nplot_histograms(data, 'Histogram of Random Variable')\n\n\n\n\nProbability theory and random variables are not just theoretical concepts; they are integral to the practical application of machine learning. Models are built upon the assumptions of probability distributions, and the visualization of data through histograms aids in model interpretation and validation. As you dive deeper into machine learning, a solid understanding of these foundational concepts will empower you to make more informed decisions in model selection, training, and evaluation.\nThe following code generates a dataset with two classes, trains a Random Forest Classifier on the data, and then generates predictions on a test set. The histograms before and after applying the machine learning model are plotted on top of one another for comparison.\nThe generate_dataset function creates a synthetic dataset with two classes, and the plot_histograms function is used to visualize the distribution before and after applying the machine learning model. The accuracy of the model is also printed to give an idea of its performance.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Function to generate a random dataset with two classes\ndef generate_dataset(seed=42):\n    np.random.seed(seed)\n    class1 = np.random.normal(loc=0, scale=1, size=500)\n    class2 = np.random.normal(loc=3, scale=1, size=500)\n    labels = [0] * 500 + [1] * 500\n    data = np.concatenate([class1, class2])\n    return data, labels\n\n# Step 1: Generate a random dataset\ndata_before, labels = generate_dataset()\n\n# Step 2: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data_before.reshape(-1, 1), labels, test_size=0.2, random_state=42)\n\n# Step 3: Train a Random Forest Classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Step 4: Make predictions on the test set\npredictions = clf.predict(X_test)\n\n# Step 5: Assess accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy of the model: {accuracy:.2%}\")\n\n# Step 6: Generate a dataset after applying the machine learning model\ndata_after = np.concatenate([clf.predict_proba(X_test)[:, 0], clf.predict_proba(X_test)[:, 1]])\n\n# Step 7: Plot histograms to compare before and after\nplot_histograms(data_before, 'Original Dataset')\nplot_histograms(data_after, 'Dataset after Machine Learning')\n\nAccuracy of the model: 89.00%"
  }
]