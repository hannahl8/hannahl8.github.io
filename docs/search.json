[
  {
    "objectID": "posts/machine-learning-intro/regression/index.html",
    "href": "posts/machine-learning-intro/regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression, a fundamental concept in machine learning, enables us to model relationships between variables. Whether predicting house prices based on square footage or analyzing the impact of advertising spend on sales, regression plays a pivotal role in understanding and predicting patterns in data."
  },
  {
    "objectID": "posts/machine-learning-intro/regression/index.html#introduction",
    "href": "posts/machine-learning-intro/regression/index.html#introduction",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Regression, a fundamental concept in machine learning, enables us to model relationships between variables. Whether predicting house prices based on square footage or analyzing the impact of advertising spend on sales, regression plays a pivotal role in understanding and predicting patterns in data."
  },
  {
    "objectID": "posts/machine-learning-intro/regression/index.html#data-visualization",
    "href": "posts/machine-learning-intro/regression/index.html#data-visualization",
    "title": "Linear and Nonlinear Regression",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\nI will use the following plot_scatter function to create the data visualizations on this page\n\nimport matplotlib.pyplot as plt\n\ndef plot_scatter(X, X_Range, ax_plot, title):\n  fig, ax = plt.subplots()\n  scatter = ax.scatter(X, y, color=blue, edgecolor='black', alpha=0.6)\n  ax.plot(X_Range, ax_plot, color=purple, linewidth=2)\n  ax.set_title(title, color=purple)\n  ax.set_xlabel('Feature', color=blue)\n  ax.set_ylabel('Target', color=blue)\n  ax.grid(True, linestyle='--', alpha=0.5)\n  ax.tick_params(axis='x', colors=blue)\n  ax.tick_params(axis='y', colors=blue)\n  ax.set_facecolor(pink)\n  fig.set_facecolor(background)\n  plt.show()\n\n\nLinear Regression\nLinear regression is a straightforward method for modeling linear relationships between variables. In a simple linear regression model, the relationship is expressed as \\(y=mx+b\\) where \\(y\\) is the target variable, \\(x\\) is the feature, \\(m\\) is the slope, and \\(b\\) is the intercept. For multiple features, the equation becomes a linear combination.\nLinear regression can be implemented effortlessly using Python’s scikit-learn library. Let’s generate a synthetic dataset and visualize the linear regression line:\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\n# Generate synthetic linear data\nnp.random.seed(42)\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\n\n# Train linear regression model\nlinear_reg = LinearRegression()\nlinear_reg.fit(X, y)\n\n# Visualize the linear regression line\nax_plot = linear_reg.predict(X)\nplot_scatter(X, X, ax_plot, 'Linear Regression')\n\n\n\n\n\n\nNonlinear Regression\nWhile linear regression is powerful, not all relationships are linear. Nonlinear regression extends the concept by accommodating more complex patterns. Polynomial regression is a common technique, where the relationship between variables is expressed as a polynomial equation.\nLet’s use Python to implement nonlinear regression through polynomial regression. This time, we’ll generate a synthetic dataset with a nonlinear relationship:\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n# Generate synthetic nonlinear data\nnp.random.seed(42)\nX = 6 * np.random.rand(100, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(100, 1)\n\n# Train nonlinear regression model (polynomial regression)\ndegree = 2\npoly_reg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\npoly_reg.fit(X, y)\n\n# Visualize the nonlinear regression curve\nX_range = np.linspace(-3, 3, 100).reshape(-1, 1)\nax_plot =  poly_reg.predict(X_range)\nplot_scatter(X, X_range, ax_plot, 'Nonlinear Regression (Polynomial)')\n\n\n\n\nThe power of regression becomes evident when we visualize the results. Scatter plots with regression lines or curves help us understand how well the model captures the underlying patterns in the data.\nIn conclusion, linear and nonlinear regression are indispensable tools in the machine learning toolbox. While linear regression is effective for simple relationships, nonlinear regression allows us to model more complex patterns. The ability to implement these techniques with Python makes them accessible and applicable to a wide range of real-world scenarios."
  },
  {
    "objectID": "posts/machine-learning-intro/intro-to-ml/index.html",
    "href": "posts/machine-learning-intro/intro-to-ml/index.html",
    "title": "Tetris and Machine Learning",
    "section": "",
    "text": "Little did I know, Tetris, the classic video game, can be related to machine learning and data analytics in various ways.\nI am a huge fan of the game, and I am obsessed with a newer version that facilitates multi-player options; Tetris® Effect: Connected.\n\n\n\n\n\nI decided to stream on twitch a few times… that didn’t go anywhere. There are several other players that I’ve seen stream online, like doremypuyotet, who is a Tetris expert; the video below is them playing.\n[]()\nAs you can see doremypuyotet has incredible technique and it is absolutely mind blowing watching them play the game.\nI wondered though, how would an Artificial Tetris Agent perform at a game of zone battle against someone like doremypuyotet? The agent would have to take several more aspects into consideration than it does in the classic video game. The dynamics of player interactions, adaptive game play, and the wealth of data generated present opportunities to enhance the gaming experience through data-driven insights and algorithmic optimization’s.\n\n\n\n\nAlgorithmic Efficiency: Tetris involves optimizing the placement of falling blocks to complete lines. This is analogous to optimization problems in machine learning where algorithms seek to minimize or maximize a certain objective function.\nHeuristics: Players often develop heuristics or rules of thumb to make decisions quickly. Similarly, in machine learning, heuristics can be used to guide algorithms toward solutions without exhaustive search.\n\n\n\nLearning from Experience: Reinforcement learning involves agents learning by interacting with an environment and receiving feedback in the form of rewards or penalties. In Tetris, players learn from the consequences of their moves, much like reinforcement learning algorithms learn from trial and error.\n\n\n\nFeature Extraction: In Tetris, players recognize patterns in the falling blocks to make decisions. In data analytics, feature extraction involves recognizing patterns in data to enhance the performance of algorithms.\nImage Recognition: Tetris can be framed as an image recognition problem where the goal is to identify patterns in the arrangement of blocks. Image recognition is a common application in machine learning.\n\n\n\nAnticipating Future States: In Tetris, players need to anticipate the future states of the game based on the current configuration of blocks. Predictive analytics in machine learning involves forecasting future trends or outcomes based on historical data.\n\n\n\nReal-time Decision Making: Tetris requires quick decision-making based on the constantly changing game state. In a similar vein, real-time data analytics involves making decisions based on rapidly changing data streams.\n\n\n\nSpatial Visualization: Tetris involves visualizing the spatial arrangement of blocks. Data visualization is a crucial aspect of data analytics, helping individuals understand complex patterns and trends in data through graphical representations.\n\n\n\nGenetic Algorithms: Tetris can be approached as a problem of evolving a strategy over time. Genetic algorithms, a type of evolutionary algorithm, involve evolving solutions to problems through processes inspired by natural selection.\n\n\n\nComplexity Analysis: Analyzing the complexity of Tetris strategies can be similar to analyzing the time and space complexity of algorithms in machine learning. Understanding the efficiency and scalability of algorithms is essential in both contexts.\n\n\n\nManaging Information Overload: Tetris speeds up as the game progresses, leading to an increasing amount of information to process. Dealing with information overload is a challenge in both Tetris and big data analytics.\n\n\n\nGame Theory: Tetris can be viewed through the lens of game theory, where players make strategic decisions to maximize their score. Game theory concepts are also applicable in various areas of machine learning, such as in competitive scenarios."
  },
  {
    "objectID": "posts/machine-learning-intro/intro-to-ml/index.html#tetris-applications-to-machine-learning",
    "href": "posts/machine-learning-intro/intro-to-ml/index.html#tetris-applications-to-machine-learning",
    "title": "Tetris and Machine Learning",
    "section": "",
    "text": "Algorithmic Efficiency: Tetris involves optimizing the placement of falling blocks to complete lines. This is analogous to optimization problems in machine learning where algorithms seek to minimize or maximize a certain objective function.\nHeuristics: Players often develop heuristics or rules of thumb to make decisions quickly. Similarly, in machine learning, heuristics can be used to guide algorithms toward solutions without exhaustive search.\n\n\n\nLearning from Experience: Reinforcement learning involves agents learning by interacting with an environment and receiving feedback in the form of rewards or penalties. In Tetris, players learn from the consequences of their moves, much like reinforcement learning algorithms learn from trial and error.\n\n\n\nFeature Extraction: In Tetris, players recognize patterns in the falling blocks to make decisions. In data analytics, feature extraction involves recognizing patterns in data to enhance the performance of algorithms.\nImage Recognition: Tetris can be framed as an image recognition problem where the goal is to identify patterns in the arrangement of blocks. Image recognition is a common application in machine learning.\n\n\n\nAnticipating Future States: In Tetris, players need to anticipate the future states of the game based on the current configuration of blocks. Predictive analytics in machine learning involves forecasting future trends or outcomes based on historical data.\n\n\n\nReal-time Decision Making: Tetris requires quick decision-making based on the constantly changing game state. In a similar vein, real-time data analytics involves making decisions based on rapidly changing data streams.\n\n\n\nSpatial Visualization: Tetris involves visualizing the spatial arrangement of blocks. Data visualization is a crucial aspect of data analytics, helping individuals understand complex patterns and trends in data through graphical representations.\n\n\n\nGenetic Algorithms: Tetris can be approached as a problem of evolving a strategy over time. Genetic algorithms, a type of evolutionary algorithm, involve evolving solutions to problems through processes inspired by natural selection.\n\n\n\nComplexity Analysis: Analyzing the complexity of Tetris strategies can be similar to analyzing the time and space complexity of algorithms in machine learning. Understanding the efficiency and scalability of algorithms is essential in both contexts.\n\n\n\nManaging Information Overload: Tetris speeds up as the game progresses, leading to an increasing amount of information to process. Dealing with information overload is a challenge in both Tetris and big data analytics.\n\n\n\nGame Theory: Tetris can be viewed through the lens of game theory, where players make strategic decisions to maximize their score. Game theory concepts are also applicable in various areas of machine learning, such as in competitive scenarios."
  },
  {
    "objectID": "posts/machine-learning-intro/classification/index.html",
    "href": "posts/machine-learning-intro/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification, a cornerstone of machine learning, empowers systems to make informed decisions based on input features. From determining whether an email is spam to diagnosing diseases, classification algorithms play a pivotal role in automating decision-making processes.\n\n\nThere are several classification algorithms, each suited to different types of problems:\n\nLogistic Regression: Ideal for binary classification tasks.\nDecision Trees: Effective for both binary and multiclass classification.\nSupport Vector Machines (SVM): Robust for linear and nonlinear classification.\n\nLet’s implement a simple classification model using logistic regression in Python:\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n\n# Generate synthetic classification data\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = (X &gt; 0.5).astype(int).flatten()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_proba = logreg.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "posts/machine-learning-intro/classification/index.html#introduction",
    "href": "posts/machine-learning-intro/classification/index.html#introduction",
    "title": "Classification",
    "section": "",
    "text": "Classification, a cornerstone of machine learning, empowers systems to make informed decisions based on input features. From determining whether an email is spam to diagnosing diseases, classification algorithms play a pivotal role in automating decision-making processes.\n\n\nThere are several classification algorithms, each suited to different types of problems:\n\nLogistic Regression: Ideal for binary classification tasks.\nDecision Trees: Effective for both binary and multiclass classification.\nSupport Vector Machines (SVM): Robust for linear and nonlinear classification.\n\nLet’s implement a simple classification model using logistic regression in Python:\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n\n# Generate synthetic classification data\nnp.random.seed(42)\nX = np.random.rand(100, 1)\ny = (X &gt; 0.5).astype(int).flatten()\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a logistic regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict probabilities on the test set\ny_proba = logreg.predict_proba(X_test)[:, 1]"
  },
  {
    "objectID": "posts/machine-learning-intro/classification/index.html#data-visualization",
    "href": "posts/machine-learning-intro/classification/index.html#data-visualization",
    "title": "Classification",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\n\nReceiver Operating Characteristic (ROC) Curve\nROC curves visualize the trade-off between true positive rate (sensitivity) and false positive rate. The area under the ROC curve (AUC-ROC) is a valuable metric for model performance.\n\nimport matplotlib.pyplot as plt\n\n# Visualize the ROC curve\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nfig, ax = plt.subplots()\nax.plot(fpr, tpr, color=blue, lw=2, label='ROC curve')\nax.plot([0, 1], [0, 1], color=purple, lw=2, linestyle='--', label='Random Guess')\nax.set_xlabel('False Positive Rate', color=blue)\nax.set_ylabel('True Positive Rate', color=blue)\nax.set_title('Receiver Operating Characteristic (ROC) Curve', color=purple)\nax.legend(loc='lower right')\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\n\n\nPrecision-Recall (PR) Curve\nPR curves focus on the trade-off between precision and recall, particularly valuable in imbalanced datasets.\n\n# Visualize the Precision-Recall curve\nprecision, recall, _ = precision_recall_curve(y_test, y_proba)\nfig, ax = plt.subplots()\nax.plot(recall, precision, color=blue, lw=2, label='Precision-Recall curve')\nax.set_xlabel('Recall (Sensitivity)', color=blue)\nax.set_ylabel('Precision', color=blue)\nax.set_title('Precision-Recall Curve', color=purple)\nax.legend(loc='lower left')\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\n\n\nConfusion Matrix\nThe confusion matrix provides a detailed understanding of a classification model’s performance, breaking down predictions into true positives, true negatives, false positives, and false negatives.\n\nfrom sklearn.metrics import confusion_matrix\n\n# Generate predictions\ny_pred = logreg.predict(X_test)\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualize the confusion matrix\nfig, ax = plt.subplots()\ncax = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.cool)\nax.set_title('Confusion Matrix', color=purple)\nplt.colorbar(cax)\nclasses = ['Class 0', 'Class 1']\ntick_marks = np.arange(len(classes))\nax.set_xticks(tick_marks)\nax.set_yticks(tick_marks)\nax.set_xticklabels(classes, rotation=45, color=blue)\nax.set_yticklabels(classes, color=blue)\nax.set_xlabel('Predicted label', color=blue)\nax.set_ylabel('True label', color=blue)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\nIn conclusion, classification in machine learning is a powerful tool for automating decision-making processes. By implementing and evaluating classification models, we gain insights into their performance through metrics like ROC curves, PR curves, and confusion matrices. These visualizations provide a nuanced understanding of a model’s strengths and weaknesses, facilitating informed decision-making in real-world applications."
  },
  {
    "objectID": "posts/diy/index.html",
    "href": "posts/diy/index.html",
    "title": "Keezer Build",
    "section": "",
    "text": "I just gotta show her off"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a Software Developer working for Naval Surface Warfare Center, Dahlgren Division. I work on an agile development team creating and improving features for a modeling and simulation software application. I am super passionate about front and back end web development (full stack), as well as general software development for desktop applications, etc. Below are some projects I have worked on during my academic career."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Keezer Build\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\nSep 30, 2023\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\n\n\n\n\n\nTetris and Machine Learning\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Series",
    "section": "",
    "text": "Series\nmachine-learning-series\nThis series contains a great deal of code and data visualizations for anyone new to machine learning.\n\n\nAll Blog Posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nKeezer Build\n\n\n\n\n\n\nDIY\n\n\nHomebrewing\n\n\n\nDetailed instructions on how I built my Keezer!\n\n\n\n\n\nMay 31, 2024\n\n\n6 min\n\n\n\n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\ncode\n\n\n\nUnveiling Anomalies with Outlier Detection in Machine Learning\n\n\n\n\n\nNov 20, 2023\n\n\n2 min\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\ncode\n\n\n\nROC, PR, Confusion Matrix\n\n\n\n\n\nOct 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\ncode\n\n\n\nNavigating Trends with Linear and Nonlinear Regression in Machine Learning\n\n\n\n\n\nOct 20, 2023\n\n\n3 min\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\ncode\n\n\n\nUnraveling Patterns with Clustering in Machine Learning\n\n\n\n\n\nSep 30, 2023\n\n\n2 min\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\ncode\n\n\n\nUnderstanding Probability Theory and Random Variables in the Context of Machine Learning\n\n\n\n\n\nSep 20, 2023\n\n\n4 min\n\n\n\n\n\n\n\nTetris and Machine Learning\n\n\n\n\n\n\ntetris\n\n\n\nMachine Learning and its applications to the classic videogame: Tetris\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "machine-learning-series.html",
    "href": "machine-learning-series.html",
    "title": "Series: Introductory Machine Learning",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\nUnveiling Anomalies with Outlier Detection in Machine Learning\n\n\n\n\n\nNov 20, 2023\n\n\n2 min\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\nROC, PR, Confusion Matrix\n\n\n\n\n\nOct 30, 2023\n\n\n3 min\n\n\n\n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\nNavigating Trends with Linear and Nonlinear Regression in Machine Learning\n\n\n\n\n\nOct 20, 2023\n\n\n3 min\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\nUnraveling Patterns with Clustering in Machine Learning\n\n\n\n\n\nSep 30, 2023\n\n\n2 min\n\n\n\n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\nUnderstanding Probability Theory and Random Variables in the Context of Machine Learning\n\n\n\n\n\nSep 20, 2023\n\n\n4 min\n\n\n\n\n\n\n\nTetris and Machine Learning\n\n\n\n\n\nMachine Learning and its applications to the classic videogame: Tetris\n\n\n\n\n\nSep 1, 2023\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/machine-learning-intro/anomaly/index.html",
    "href": "posts/machine-learning-intro/anomaly/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection, a critical aspect of machine learning, involves identifying patterns in data that deviate significantly from the norm. This technique finds applications in various domains, from fraud detection in financial transactions to detecting anomalies in network traffic for cybersecurity.\n\n\nWhile anomaly detection is a powerful tool, it comes with challenges. Real-world data often exhibits diverse distributions, and datasets can be imbalanced, making it tricky to identify rare anomalies accurately. Robust algorithms are essential to overcome these challenges."
  },
  {
    "objectID": "posts/machine-learning-intro/anomaly/index.html#introduction",
    "href": "posts/machine-learning-intro/anomaly/index.html#introduction",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection, a critical aspect of machine learning, involves identifying patterns in data that deviate significantly from the norm. This technique finds applications in various domains, from fraud detection in financial transactions to detecting anomalies in network traffic for cybersecurity.\n\n\nWhile anomaly detection is a powerful tool, it comes with challenges. Real-world data often exhibits diverse distributions, and datasets can be imbalanced, making it tricky to identify rare anomalies accurately. Robust algorithms are essential to overcome these challenges."
  },
  {
    "objectID": "posts/machine-learning-intro/anomaly/index.html#data-visualization",
    "href": "posts/machine-learning-intro/anomaly/index.html#data-visualization",
    "title": "Anomaly/Outlier Detection",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\n\nDBSCAN and Evaluation Metrics for Anomaly Detection\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) emerges as a robust algorithm for anomaly detection. Unlike traditional methods, DBSCAN doesn’t assume a specific shape for clusters and can effectively isolate points in low-density regions as outliers.\nVisualizing anomalies is crucial for interpreting model results. Scatter plots with DBSCAN labels provide an intuitive representation of the identified anomalies in the dataset.\nEvaluating anomaly detection models poses challenges due to the scarcity of anomalies. Metrics such as precision, recall, and F1 score provide a nuanced understanding of the model’s performance.\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Function to generate a synthetic dataset with outliers\ndef generate_dataset_with_outliers(seed=42):\n    np.random.seed(seed)\n    X, _ = make_blobs(n_samples=300, centers=1, cluster_std=1, random_state=0)\n    outliers = np.random.uniform(low=-10, high=10, size=(20, 2))\n    return np.concatenate([X, outliers]), np.concatenate([np.zeros(300), -np.ones(20)])\n\n# Step 1: Generate a synthetic dataset with outliers\nX, y_true = generate_dataset_with_outliers()\n\n# Step 2: Train a DBSCAN model for anomaly detection\ndbscan = DBSCAN(eps=1, min_samples=5)\npredicted_labels = dbscan.fit_predict(X)\n\n# Step 3: Visualize anomalies with DBSCAN labels\nfig, ax = plt.subplots()\nscatter = ax.scatter(X[:, 0], X[:, 1], c=predicted_labels, cmap=plt.cm.cool, edgecolor='black', s=50)\nax.set_title('Anomaly Detection with DBSCAN', color=purple)\nax.set_xlabel('Feature 1', color=blue)\nax.set_ylabel('Feature 2', color=blue)\nax.grid(True, linestyle='--', alpha=0.5)\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n# Step 4: Evaluate the model using precision, recall, and F1 score\nprecision = precision_score(y_true, predicted_labels, pos_label=-1)\nrecall = recall_score(y_true, predicted_labels, pos_label=-1)\nf1 = f1_score(y_true, predicted_labels, pos_label=-1)\n\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\n\n\n\n\n\n\n\nPrecision: 1.00\nRecall: 0.90\nF1 Score: 0.95\n\n\nAnomaly detection, powered by algorithms like DBSCAN, plays a vital role in uncovering irregularities in data. The combination of effective visualizations and evaluation metrics enables us to gain insights into the anomalies present, providing a foundation for critical decision-making in various domains."
  },
  {
    "objectID": "posts/machine-learning-intro/clustering/index.html",
    "href": "posts/machine-learning-intro/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering, a fundamental technique in machine learning, is the process of grouping similar data points together. Its applications are diverse, ranging from customer segmentation in marketing to anomaly detection in cybersecurity. By identifying patterns within datasets, clustering enables us to gain valuable insights and make informed decisions.\n\n\nClustering algorithms come in various flavors, each with its unique approach to grouping data:\n\nK-Means: Divides data into k clusters, where each cluster is represented by its centroid.\nHierarchical Clustering: Forms a tree of clusters, allowing for hierarchical organization.\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): A density-based algorithm that identifies clusters based on the density of data points.\n\nDBSCAN stands out for its ability to discover clusters of arbitrary shapes. The algorithm works by identifying core points, which are densely packed, and expanding clusters by connecting core points. It also identifies noise points that do not belong to any cluster."
  },
  {
    "objectID": "posts/machine-learning-intro/clustering/index.html#introduction",
    "href": "posts/machine-learning-intro/clustering/index.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "Clustering, a fundamental technique in machine learning, is the process of grouping similar data points together. Its applications are diverse, ranging from customer segmentation in marketing to anomaly detection in cybersecurity. By identifying patterns within datasets, clustering enables us to gain valuable insights and make informed decisions.\n\n\nClustering algorithms come in various flavors, each with its unique approach to grouping data:\n\nK-Means: Divides data into k clusters, where each cluster is represented by its centroid.\nHierarchical Clustering: Forms a tree of clusters, allowing for hierarchical organization.\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): A density-based algorithm that identifies clusters based on the density of data points.\n\nDBSCAN stands out for its ability to discover clusters of arbitrary shapes. The algorithm works by identifying core points, which are densely packed, and expanding clusters by connecting core points. It also identifies noise points that do not belong to any cluster."
  },
  {
    "objectID": "posts/machine-learning-intro/clustering/index.html#data-visualization",
    "href": "posts/machine-learning-intro/clustering/index.html#data-visualization",
    "title": "Clustering",
    "section": "Data Visualization",
    "text": "Data Visualization\nLet’s demonstrate the power of DBSCAN using Python and scikit-learn. We’ll start by generating a synthetic dataset:\n\nimport numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# Generate a synthetic dataset\nX, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.3, min_samples=5)\nlabels = dbscan.fit_predict(X)\n\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\nVisualizing the results of clustering is crucial for understanding the underlying structure of the data. Let’s create a scatter plot that represents the original data points with cluster labels:\n\nimport matplotlib.pyplot as plt\n\n# Visualize the clustering\nfig, ax = plt.subplots()\nscatter = ax.scatter(X[:, 0], X[:, 1], c=labels, cmap=plt.cm.cool, edgecolor='black', s=50)\nax.set_title('DBSCAN Clustering', color=purple)\nax.set_xlabel('Feature 1', color=blue)\nax.set_ylabel('Feature 2', color=blue)\nax.grid(True, linestyle='--', alpha=0.5)\nax.tick_params(axis='x', colors=blue)\nax.tick_params(axis='y', colors=blue)\nax.set_facecolor(pink)\nfig.set_facecolor(background)\nplt.show()\n\n\n\n\nClustering, especially with the powerful DBSCAN algorithm, opens new avenues for extracting patterns from data. As we’ve seen, DBSCAN’s ability to identify clusters based on density makes it versatile for a wide range of applications. By visualizing the results, we gain a deeper understanding of the data’s inherent structure, providing a solid foundation for subsequent analysis and decision-making."
  },
  {
    "objectID": "posts/machine-learning-intro/probability/index.html",
    "href": "posts/machine-learning-intro/probability/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In the realm of machine learning, uncertainty is inherent. Models make predictions based on patterns learned from data, and probability theory provides a formal framework for dealing with uncertainty. Random variables are employed to represent uncertain quantities, making probability theory a cornerstone for building and interpreting machine learning models.\nMachine learning algorithms often assume specific probability distributions. For instance:\n\nThe normal distribution is frequently assumed in linear regression and Gaussian processes.\nThe binomial distribution is relevant for problems involving binary outcomes, such as classification tasks.\n\nUnderstanding the characteristics of these distributions is vital when selecting appropriate models and interpreting results.\nRandom variables represent various aspects of data. Features can be modeled as random variables, and understanding their distribution is crucial for feature engineering. Additionally, the uncertainty associated with predictions is often expressed through probability distributions, making random variables central to the prediction process."
  },
  {
    "objectID": "posts/machine-learning-intro/probability/index.html#introduction",
    "href": "posts/machine-learning-intro/probability/index.html#introduction",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "In the realm of machine learning, uncertainty is inherent. Models make predictions based on patterns learned from data, and probability theory provides a formal framework for dealing with uncertainty. Random variables are employed to represent uncertain quantities, making probability theory a cornerstone for building and interpreting machine learning models.\nMachine learning algorithms often assume specific probability distributions. For instance:\n\nThe normal distribution is frequently assumed in linear regression and Gaussian processes.\nThe binomial distribution is relevant for problems involving binary outcomes, such as classification tasks.\n\nUnderstanding the characteristics of these distributions is vital when selecting appropriate models and interpreting results.\nRandom variables represent various aspects of data. Features can be modeled as random variables, and understanding their distribution is crucial for feature engineering. Additionally, the uncertainty associated with predictions is often expressed through probability distributions, making random variables central to the prediction process."
  },
  {
    "objectID": "posts/machine-learning-intro/probability/index.html#data-visualization",
    "href": "posts/machine-learning-intro/probability/index.html#data-visualization",
    "title": "Probability Theory and Random Variables",
    "section": "Data Visualization",
    "text": "Data Visualization\nI use the following colors in all of my blogs data visualizations\n\n# Define specific colors (same as CSS from quarto vapor theme)\nbackground = '#1b133a'\npink = '#ea39b8'\npurple = '#6f42c1'\nblue = '#32fbe2'\n\nI will use the following plot_histograms function to create the data visualizations on this page\n\nimport matplotlib.pyplot as plt\n\n# Function to plot histograms\ndef plot_histograms(data, title):\n    # Create a histogram\n    fig, ax = plt.subplots()\n    hist = ax.hist(data, bins=30, edgecolor='black', alpha=0.7, color=purple, ec=blue)\n    ax.set_title(title, color=purple)\n    ax.set_xlabel('Values', color=blue)\n    ax.set_ylabel('Frequency', color=blue)\n    ax.grid(True, linestyle='--', alpha=0.5)\n    ax.tick_params(axis='x', colors=blue)\n    ax.tick_params(axis='y', colors=blue)\n    \n    ax.set_facecolor(pink)\n    fig.set_facecolor(background)\n    \n    # Show the plot\n    plt.show()\n\nLet’s use Python to create a histogram of a simulated random variable:\n\nimport numpy as np\n\n# Sets the seed for the random number generator to 42\nnp.random.seed(42)\n# Generates an array of 1000 random numbers drawn from a normal distribution\n# using the random number generated defined above\ndata = np.random.normal(size=1000)\nplot_histograms(data, 'Histogram of Random Variable')\n\n\n\n\nProbability theory and random variables are not just theoretical concepts; they are integral to the practical application of machine learning. Models are built upon the assumptions of probability distributions, and the visualization of data through histograms aids in model interpretation and validation. As you dive deeper into machine learning, a solid understanding of these foundational concepts will empower you to make more informed decisions in model selection, training, and evaluation.\nThe following code generates a dataset with two classes, trains a Random Forest Classifier on the data, and then generates predictions on a test set. The histograms before and after applying the machine learning model are plotted on top of one another for comparison.\nThe generate_dataset function creates a synthetic dataset with two classes, and the plot_histograms function is used to visualize the distribution before and after applying the machine learning model. The accuracy of the model is also printed to give an idea of its performance.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Function to generate a random dataset with two classes\ndef generate_dataset(seed=42):\n    np.random.seed(seed)\n    class1 = np.random.normal(loc=0, scale=1, size=500)\n    class2 = np.random.normal(loc=3, scale=1, size=500)\n    labels = [0] * 500 + [1] * 500\n    data = np.concatenate([class1, class2])\n    return data, labels\n\n# Step 1: Generate a random dataset\ndata_before, labels = generate_dataset()\n\n# Step 2: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data_before.reshape(-1, 1), labels, test_size=0.2, random_state=42)\n\n# Step 3: Train a Random Forest Classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n# Step 4: Make predictions on the test set\npredictions = clf.predict(X_test)\n\n# Step 5: Assess accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy of the model: {accuracy:.2%}\")\n\n# Step 6: Generate a dataset after applying the machine learning model\ndata_after = np.concatenate([clf.predict_proba(X_test)[:, 0], clf.predict_proba(X_test)[:, 1]])\n\n# Step 7: Plot histograms to compare before and after\nplot_histograms(data_before, 'Original Dataset')\nplot_histograms(data_after, 'Dataset after Machine Learning')\n\nAccuracy of the model: 89.00%"
  },
  {
    "objectID": "posts/diy/index.html#table-of-eveer",
    "href": "posts/diy/index.html#table-of-eveer",
    "title": "My DIY Keezer Build!",
    "section": "Table of eveer",
    "text": "Table of eveer"
  },
  {
    "objectID": "posts/diy/index.html#table-of-everything-i-purchased",
    "href": "posts/diy/index.html#table-of-everything-i-purchased",
    "title": "My DIY Keezer Build!",
    "section": "Table of everything I purchased",
    "text": "Table of everything I purchased\n\n\n\n\n\n\n\nItem with link\nPrice\n\n\n\n\n5.0 cu. ft. Chest Freezer\n$169.00\n\n\n3” Stainless Steel Dual Faucet Draft Tower\n$183.99\n\n\nRed Oak Plywood 3/4 in. x 2 ft. x 4 ft. (2x)\n$58.98\n\n\nRed Oak Lumbar 1-in x 3-in x 6-ft (2x)\n$34.96\n\n\n\n$.00\n\n\n\n$.00\n\n\n\n$.00\n\n\n\n$.00\n\n\n\n$.00\n\n\n\n$.00\n\n\n\n$.00\n\n\n\n$.00\n\n\n\n$.00"
  },
  {
    "objectID": "timeline.html",
    "href": "timeline.html",
    "title": "My Academic & Career Timeline",
    "section": "",
    "text": "Descending\n\n\n\n\n\nSeptember 2024 - Present\n\nComputer Scientist at Naval Surface Warfare Center Dahlgren Division\n\n\nI am a software developer on a agile team creating and improving features for a modeling and simulation software application. Other responsibilities included leading code reviews, menotring junior developers, and contributing to technical design decisions.\n\n\n\n\n\nAugust 2024\n\nConferred the Master of Engineering\n\n\nConferred the Master of Engineering. Major – Computer Science and Applications on August 9th, 2024. Capstone Project showcased on VTechWorks.\n\n\n\n\n\nAugust 2023\n\nGraduate Student at Virginia Tech Innovation Campus\n\n\nI began as a graduate student at Virginia Tech. I was awarded the Boeing Graduate Scholarship which was highlighted on Virginia Tech News.\n\n\n\n\n\nJune 2021 - September 2024\n\nComputer Scientist at Naval Surface Warfare Center Indian Head Division\n\n\nI worked as the lead Software Developer on the Modeling and Simulations (M&S) Team. I worked with a team of developers on the creation of a Java-based software application over a multi-year period. Led the project management for several high–intensity, short duration funded efforts, delivering M&S support as requested by upper-level management.\n\n\n\n\n\nMay 2021\n\nConferred the Bachelor of Science\n\n\nConferred the Bachelor of Science. Major – Biochemistry. Magna Cum Laude, on May 15, 2021. Completed the Minor in Chemistry. Completed the Minor in Computer Science.\n\n\n\n\n\nJune 2020 - August 2020 & December 2020 - January 2021\n\nStudent Intern at Naval Surface Warfare Center Indian Head Division\n\n\nStarted as a student intern working with the Modeling & Simulations Team for Chemical, Biological, and Radiological (CBR) Defense. During my time as an intern, I conducted a comprehensive literature review and addressed foundational questions contributing to the successful initiation of their newly 3-year funded software application.\n\n\n\n\n\nAugust 2019\n\nComputer Science Minor\n\n\nI really liked biochemistry as a major (mostly just the chemistry classes), but I still wanted to broaden my knowledge! I realized I had enough open electives left to add a minor, and computer science was calling to me. The first class I ever took was an insane learning curve, but also an incredibly rewarding challenge. I knew after successfully completing that class, to have a career in the field of computer science would be beyond gratifying.\n\n\n\n\n\nMay 2019 - August 2019\n\nBookkeeper at Koons Chevy Buick GMC\n\n\nI worked in the accounting department as a bookkeeper. My daily tasks included things like preparing and handling legal data for vehicle registration and taxation. Lot’s of paper work! The car industry loves paper!\n\n\n\n\n\nAugust 2017\n\nUndergraduate Student at Virginia Tech\n\n\nI began as an undergraduate student at Virgina Tech. Many major changes later until I decided on Biochemestry! General Chemisty was my favorite class!\n\n\n\n\n\nJune 2017\n\nGraduated High School\n\n\nHappy to be done! I worked the rest of the summer to make money before starting college in August.\n\n\n\n\n\nMarch 2016 - August 2017\n\nStarted my First Job!\n\n\nStarted as an Associate at Potbelly Sandwich Works. Quickly promoted to be a certified trainer, training all new employees. Learned valuble teamwork and customer service skills."
  },
  {
    "objectID": "timeline.html#timeline-of-accomplishments",
    "href": "timeline.html#timeline-of-accomplishments",
    "title": "My Accomplishments Timeline",
    "section": "",
    "text": "August 2017\n\n\nI began as an undergraduate student at Virgina Tech. Many major changes later until I decided on Biochemestry! General Chemisty was my favorite class!\n\n\n\n\n\n\n\nMay 2019 - August 2019\n\n\nxxx"
  },
  {
    "objectID": "index.html#my-academic-career-timeline",
    "href": "index.html#my-academic-career-timeline",
    "title": "About",
    "section": "My Academic & Career Timeline",
    "text": "My Academic & Career Timeline\n\n\n\nDescending\n\n\n\n\n\nSeptember 2024 - Present\n\nComputer Scientist at Naval Surface Warfare Center Dahlgren Division\n\n\nLead software developer on an agile development team creating and improving features for a modeling and simulation software application. Other responsibilities include leading code reviews, mentoring junior developers, and contributing to technical design decisions.\n\n\n\n\n\nAugust 2024\n\nConferred the Master of Engineering\n\n\nConferred the Master of Engineering. Major – Computer Science and Applications on August 9th, 2024. Capstone Project showcased on VTechWorks.\n\n\n\n\n\nAugust 2023\n\nGraduate Student at Virginia Tech Innovation Campus\n\n\nI began as a graduate student at Virginia Tech. I was awarded the Boeing Graduate Scholarship which was highlighted on Virginia Tech News.\n\n\n\n\n\nJune 2021 - September 2024\n\nComputer Scientist at Naval Surface Warfare Center Indian Head Division\n\n\nI worked as the lead Software Developer on the Modeling and Simulations (M&S) Team. I worked with a team of developers on the creation of a Java-based software application over a multi-year period. Led the project management for several high–intensity, short duration funded efforts, delivering M&S support as requested by upper-level management.\n\n\n\n\n\nMay 2021\n\nConferred the Bachelor of Science\n\n\nConferred the Bachelor of Science. Major – Biochemistry. Magna Cum Laude, on May 15, 2021. Completed the Minor in Chemistry. Completed the Minor in Computer Science.\n\n\n\n\n\nJune 2020 - August 2020 & December 2020 - January 2021\n\nStudent Intern at Naval Surface Warfare Center Indian Head Division\n\n\nStarted as a student intern working with the Modeling & Simulations Team for Chemical, Biological, and Radiological (CBR) Defense. During my time as an intern, I conducted a comprehensive literature review and addressed foundational questions contributing to the successful initiation of their newly funded 3-year software development effors.\n\n\n\n\n\nAugust 2019\n\nComputer Science Minor\n\n\nI really liked biochemistry as a major (mostly just the chemistry classes), but I still wanted to broaden my knowledge! I realized I had enough open electives left to add a minor, and computer science was calling to me. The first class I ever took was an insane learning curve, but also an incredibly rewarding challenge. I knew after successfully completing that class, to have a career in the field of computer science would be beyond gratifying.\n\n\n\n\n\nMay 2019 - August 2019\n\nBookkeeper at Koons Chevy Buick GMC\n\n\nI worked in the accounting department as a bookkeeper. My daily tasks included things like preparing and handling legal data for vehicle registration and taxation. Lot’s of paper work! The car industry loves paper!\n\n\n\n\n\nAugust 2017\n\nUndergraduate Student at Virginia Tech\n\n\nI began as an undergraduate student at Virgina Tech. Many major changes later until I decided on Biochemestry! General Chemisty was my favorite class!\n\n\n\n\n\nJune 2017\n\nGraduated High School\n\n\nHappy to be done! I worked the rest of the summer to make money before starting college in August.\n\n\n\n\n\nMarch 2016 - August 2017\n\nStarted my First Job!\n\n\nStarted as an Associate at Potbelly Sandwich Works. Quickly promoted to be a certified trainer, training all new employees. Learned valuble teamwork and customer service skills."
  },
  {
    "objectID": "index.html#project-links",
    "href": "index.html#project-links",
    "title": "About",
    "section": "Project Links",
    "text": "Project Links\n\n\n\nHeader\n\n\n\nPrimary Card title\n\n\nSome quick example text!\n\n\n\n\nHokie Housing Hub\nfocal\nBlog"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "About",
    "section": "Projects",
    "text": "Projects\n\n\n\n\n\n\n\nProject Link\nSkills/Resources/Languages Used\n\n\n\n\nVT Research Connect\nReact, Typescript, Vite, SQL, Java, Jira, Python\n\n\nHokie Housing Hub\nHTML, CSS, JavaScript\n\n\nFocal\ndJango, python, HTML, CSS, JavaScript\n\n\nPersonal Blog\nQuarto, R, HTML, CSS, JavaScript"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "About",
    "section": "",
    "text": "Header\n\n\nPrimary Card title\n\n\nSome quick example text!"
  },
  {
    "objectID": "index.html#section-1",
    "href": "index.html#section-1",
    "title": "About",
    "section": "",
    "text": "Header\n\n\nPrimary Card title\n\n\nSome quick example text!\n\n\n\nHokie Housing Hub\nfocal\nBlog"
  },
  {
    "objectID": "posts/diy/index.html#paining-the-chest-freezer",
    "href": "posts/diy/index.html#paining-the-chest-freezer",
    "title": "Keezer Build",
    "section": "Paining the Chest Freezer",
    "text": "Paining the Chest Freezer\nPainting the chest freezer was definitely the easiest part. First I sanded the whole thing and put painters tape around the seal, drain hole, and the little green light that shows the freezer is on.\n\n\n\n\n\nThen I primed the freezer, I didn’t prime the top because I wouldn’t be painting that. I knew I was going to be adding the tabletop later on.\n\n\n\n\n\nA day or two later I pained it this lovely blue color. The lid is also now removed because I was prepping for the tabletop to be added."
  },
  {
    "objectID": "posts/diy/index.html#connecting-the-wooden-tabletop-and-the-freezer-lid",
    "href": "posts/diy/index.html#connecting-the-wooden-tabletop-and-the-freezer-lid",
    "title": "Keezer Build",
    "section": "Connecting the Wooden Tabletop and the Freezer Lid",
    "text": "Connecting the Wooden Tabletop and the Freezer Lid\nThe first thing I had to do was drill a hole in the freezer lid, I used a hole saw to do this. Unfortunately, I drilled right through the wires that connected to the freezer light in the lid, but I carefully removed them so it wasn’t an issue. And I didn’t need the light anyways so I wasn’t upset about it.\n\n\n\n\n\nNext I had to line up the hole in the freezer to the wooden piece. This was easy. I also added painters tape to show where the freezer lid will go.\n\n\n\n\n\nHere I laid out the 1/2” washers to go in between the wood and the freezer lid. This was because the lid had the smallest dip on the edges raising it, and I needed this lid to be as secure as possible to the wood.\n\n\n\n\n\nFinally here’s everything glued on to the wood and ready to have to freezer lid placed on top.\n\n\n\n\n\nHere’s the lid glued on and with additional washers that I screwed through the freezer lid into the wood. I let this set for at least 48 hours before messing with it."
  },
  {
    "objectID": "posts/diy/index.html#finishing-the-wooden-tabletop",
    "href": "posts/diy/index.html#finishing-the-wooden-tabletop",
    "title": "Keezer Build",
    "section": "Finishing the Wooden Tabletop",
    "text": "Finishing the Wooden Tabletop\nI had to add tabletop edges to this thing. Let me tell you, using a table saw to cut 45 degree angled edges was not easy! And table saws are actually terrifying. Here I have the three pieces of wood ready to to be attached and make this baby look more elegant.\n\n\n\n\n\nSo I am silly and only bought gray glue. I should have gotten one that matched the stain I was going to use, but I masking taped this baby up to help prevent any gray glue getting anywhere. I needed several hands for this, I was going to glue the edges on, and then also nail them in with really tiny nails.\n\n\n\n\n\nHere she is with the edges 😍 This was definitely the most stressful part of this build. It didn’t take long but I was so worried about the gluing and nailing and not having the edges aligned perfectly, but thankfully it all worked out!\n\n\n\n\n\nThere was some issues, I didn’t have any really big clamps to push the wood super close together, so there are gaps between the top and the edges, and some places you could see glue. I ended up picking out the gray glue later on and filling those edges with red oak wood filler before I stained. This helped A LOT. If I were to redo this, I would definitely get big clamps so those gaps wouldn’t be there.\n\n\n\n\n\nI also filled all the nails in with red oak wood filler before I stained as well."
  },
  {
    "objectID": "posts/diy/index.html#staining-the-tabletop",
    "href": "posts/diy/index.html#staining-the-tabletop",
    "title": "Keezer Build",
    "section": "Staining the Tabletop",
    "text": "Staining the Tabletop\nI obviously sanded the crap out of this before I stained. I LOVED how this stain turned out. I love the grain pattern for red oak wood, it is a pricier wood but it is just so beautiful.\n\n\n\n\n\nHere is after the second stain.\n\n\n\n\n\nI did some final touch ups with the red oak wood filler, did one more stain, and finished with polyurethane."
  },
  {
    "objectID": "posts/diy/index.html#making-the-logo",
    "href": "posts/diy/index.html#making-the-logo",
    "title": "Keezer Build",
    "section": "Making the Logo",
    "text": "Making the Logo\nThe logo is probably my favorite part. When I started dating my boyfriend (Liam) back in 2020, I told him I wanted to have my own brewery one day. He doodled up an amazing logo.\n\n\n\n\n\nFour years later, he recreated it using Adobe Illustrator, and he conveniently works at a print shop and printed the logo directly on to aluminum dibond."
  },
  {
    "objectID": "posts/diy/index.html#finishing-touches",
    "href": "posts/diy/index.html#finishing-touches",
    "title": "Keezer Build",
    "section": "Finishing Touches",
    "text": "Finishing Touches\nFinally after the wood was done drying (I waited 1 week) I attached the tap tower. I then glued small wooden pieces to the front of the freezer. Then I took the circular stained piece of wood (Liam got everything cut into circles at his job, I didn’t have a tool for that) and glued that onto the smaller pieces of wood to make it look elevated off the wood. I also know you’re really not supposed to have anything covering a freezer, because it needs to release heat, so I didn’t want the big circular piece of wood directly glued onto the freezer. Then I just glued the logo onto the piece of wood!\n\n\n\n\n\nFinally I added LED lights underneath the lid to illuminate this masterpiece at night!\n\n\n\n\n\nThen I added all the keg elements into the freezer, along with attaching the Inkbird temperature controller to keep the freezer at usually around 34 degrees, I like a crisp cold beer 😎\n\n\n\n\n\nAnd finally, at night 😏"
  }
]